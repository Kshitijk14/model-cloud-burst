{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, BatchNormalization, Dropout, Dense, GRU, Conv1D, MaxPooling1D, Flatten, Reshape \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    try:\n",
    "        # Process temporal features\n",
    "        df['Seconds'] = df.index.map(pd.Timestamp.timestamp)\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = (365.2425) * day\n",
    "        \n",
    "        # Add cyclical time features\n",
    "        df['Day sin'] = np.sin(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Day cos'] = np.cos(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Year sin'] = np.sin(df['Seconds'] * (2 * np.pi / year))\n",
    "        df['Year cos'] = np.cos(df['Seconds'] * (2 * np.pi / year))\n",
    "        \n",
    "        # # Add rolling statistics\n",
    "        # df['rolling_mean_6h'] = df.iloc[:, 0].rolling(window=6).mean()\n",
    "        # df['rolling_std_6h'] = df.iloc[:, 0].rolling(window=6).std()\n",
    "        # df['rolling_max_6h'] = df.iloc[:, 0].rolling(window=6).max()\n",
    "        \n",
    "        # # Add lag features\n",
    "        # for i in [1, 3, 6, 12]:\n",
    "        #     df[f'lag_{i}'] = df.iloc[:, 0].shift(i)\n",
    "        \n",
    "        df = df.bfill()\n",
    "        df = df.drop(['Seconds'], axis=1)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def df_to_X_y(df, window_size=24):\n",
    "    try:\n",
    "        df_as_np = df.to_numpy()\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(len(df_as_np) - window_size):\n",
    "            row = [r for r in df_as_np[i: i + window_size]]\n",
    "            X.append(row)\n",
    "            \n",
    "            label = df_as_np[i + window_size][0]\n",
    "            y.append(label)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in df_to_X_y: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_time_series_data(X, y, train_ratio=0.8, val_ratio=0.1):\n",
    "    try:\n",
    "        n = len(X)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "        X_train, y_train = X[:train_end], y[:train_end]\n",
    "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "        X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in split_time_series_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def standardize_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Standardize all features in the dataset while maintaining the 3D structure\n",
    "    (samples, timesteps, features).\n",
    "    \"\"\"\n",
    "    num_samples, num_timesteps, num_features = X_train.shape\n",
    "    \n",
    "    # Reshape to 2D for StandardScaler (combine samples & timesteps)\n",
    "    X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "    \n",
    "    # Fit the scaler on training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "    # Transform validation and test sets\n",
    "    X_val_scaled = scaler.transform(X_val.reshape(-1, num_features))\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, num_features))\n",
    "\n",
    "    # Reshape back to original 3D shape\n",
    "    X_train = X_train_scaled.reshape(num_samples, num_timesteps, num_features)\n",
    "    X_val = X_val_scaled.reshape(X_val.shape[0], X_val.shape[1], num_features)\n",
    "    X_test = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], num_features)\n",
    "\n",
    "    return X_train, X_val, X_test, scaler\n",
    "\n",
    "# Model architectures with regularization and dropout\n",
    "def create_lstm_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            LSTM(\n",
    "                128, \n",
    "                return_sequences=True, \n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            LSTM(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_lstm_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_gru_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            GRU(\n",
    "                128, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            GRU(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_gru_model: {e}\")\n",
    "        raise\n",
    "\n",
    "# def create_cnn_model(input_shape, dropout_rate=0.3):\n",
    "#     try:\n",
    "#         model = Sequential([\n",
    "#             InputLayer(shape=input_shape),\n",
    "            \n",
    "#             Conv1D(\n",
    "#                 filters=128, \n",
    "#                 kernel_size=3, \n",
    "#                 padding='same', \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             BatchNormalization(),\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "#             Dropout(dropout_rate),\n",
    "            \n",
    "#             Conv1D(\n",
    "#                 filters=64, \n",
    "#                 kernel_size=3, \n",
    "#                 padding='same', \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             BatchNormalization(),\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "#             Dropout(dropout_rate),\n",
    "#             Flatten(),\n",
    "            \n",
    "#             Dense(\n",
    "#                 32, \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             Dropout(dropout_rate),\n",
    "            \n",
    "#             Dense(1, activation='linear')\n",
    "#         ])\n",
    "#         return model\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in create_cnn_model: {e}\")\n",
    "#         raise\n",
    "\n",
    "# def create_cnn_lstm_model(input_shape, dropout_rate=0.3):\n",
    "#     try:\n",
    "#         model = Sequential([\n",
    "#             InputLayer(shape=input_shape),\n",
    "\n",
    "#             # CNN Feature Extractor\n",
    "#             Conv1D(\n",
    "#                 filters=64, \n",
    "#                 kernel_size=3, \n",
    "#                 padding='same', \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             BatchNormalization(),\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "#             Dropout(dropout_rate),\n",
    "\n",
    "#             Conv1D(\n",
    "#                 filters=128, \n",
    "#                 kernel_size=3, \n",
    "#                 padding='same', \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             BatchNormalization(),\n",
    "#             MaxPooling1D(pool_size=2),\n",
    "#             Dropout(dropout_rate),\n",
    "\n",
    "#             # Reshape for LSTM Input (time_steps, features)\n",
    "#             Reshape((input_shape[0] // 4, 128)),  # Adjust time steps based on pooling\n",
    "\n",
    "#             # LSTM Layer for Temporal Dependencies\n",
    "#             LSTM(\n",
    "#                 128, \n",
    "#                 return_sequences=True, \n",
    "#                 dropout=dropout_rate, \n",
    "#                 recurrent_dropout=dropout_rate\n",
    "#             ),\n",
    "#             LSTM(\n",
    "#                 64, \n",
    "#                 dropout=dropout_rate, \n",
    "#                 recurrent_dropout=dropout_rate\n",
    "#             ),\n",
    "\n",
    "#             Dense(\n",
    "#                 32, \n",
    "#                 activation='relu',\n",
    "#                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "#             ),\n",
    "#             Dropout(dropout_rate),\n",
    "\n",
    "#             Dense(1, activation='linear')\n",
    "#         ])\n",
    "#         return model\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in create_cnn_lstm_model: {e}\")\n",
    "#         raise\n",
    "\n",
    "# Weighted mean squared error\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    try:\n",
    "        weights = tf.where(y_pred < y_true, 2.0, 1.0) # Penalize underestimation more heavily (false negatives)\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        return tf.reduce_mean(weights * squared_difference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in weighted_mse: {e}\")\n",
    "        raise\n",
    "\n",
    "# Custom loss function to penalize false negatives more heavily\n",
    "def custom_loss(y_true, y_pred):\n",
    "    try:\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        mse = tf.reduce_mean(squared_difference, axis=-1)\n",
    "        \n",
    "        # Penalize underestimation more heavily (false negatives)\n",
    "        penalty = tf.where(y_pred < y_true, 2.0, 1.0)\n",
    "        return mse * penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_loss: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Training function with callbacks\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_path, \n",
    "                batch_size=32, epochs=5, patience=3, loss_function='mse', optimizer_function='adam', learning_rate=1e-3):\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, start_from_epoch=3),\n",
    "            ModelCheckpoint(model_path, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patience, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Select the optimizer based on the input parameter\n",
    "        if optimizer_function == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n",
    "        elif optimizer_function == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Select the loss function based on the input parameter\n",
    "        if loss_function == 'weighted_mse':\n",
    "            loss = weighted_mse\n",
    "        elif loss_function == 'custom':\n",
    "            loss = custom_loss\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                'mse', \n",
    "                'mae',\n",
    "                'root_mean_squared_error',\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Evaluation function with focus on false negatives\n",
    "# def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "#     try:\n",
    "#         predictions = model.predict(X_test)\n",
    "        \n",
    "#         # Compute MSE and MAE\n",
    "#         mse = np.mean((y_test - predictions.flatten())**2)\n",
    "#         mae = np.mean(np.abs(y_test - predictions.flatten()))\n",
    "        \n",
    "#         # Convert to binary labels\n",
    "#         binary_actual = y_test > threshold\n",
    "#         binary_pred = predictions.flatten() > threshold\n",
    "        \n",
    "#         # Compute false negatives and FNR\n",
    "#         false_negatives = np.sum((binary_actual == True) & (binary_pred == False))\n",
    "#         total_positives = np.sum(binary_actual)\n",
    "#         false_negative_rate = false_negatives / total_positives if total_positives > 0 else 0.0\n",
    "        \n",
    "#         return {\n",
    "#             'mse': mse,\n",
    "#             'mae': mae,\n",
    "#             'false_negative_rate': false_negative_rate\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in evaluate_model: {e}\")\n",
    "#         raise\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a time series forecasting model with focus on false negatives\n",
    "    and additional metrics (CSI, POD, FAR, Precision, Recall, F1-score)\n",
    "    using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained TensorFlow Keras model.\n",
    "        X_test (np.ndarray or tf.Tensor): Test data features.  (Assumed already a tensor or convertible)\n",
    "        y_test (np.ndarray or tf.Tensor): Test data labels (actual values). (Assumed already a tensor or convertible)\n",
    "        threshold (float): Threshold to define a cloudburst event.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of evaluation metrics (all TensorFlow tensors).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure X_test and y_test are TensorFlow tensors\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "        y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "        predictions = model.predict(X_test)  # model.predict returns numpy array\n",
    "\n",
    "        # Convert predictions to a TensorFlow tensor\n",
    "        predictions = tf.convert_to_tensor(predictions.flatten(), dtype=tf.float32)\n",
    "\n",
    "        # Compute MSE and MAE (using TensorFlow)\n",
    "        mse = tf.reduce_mean((y_test - predictions)**2)\n",
    "        mae = tf.reduce_mean(tf.abs(y_test - predictions))\n",
    "\n",
    "        # Compute RMSE (using TensorFlow)\n",
    "        rmse = tf.sqrt(mse)  # RMSE is the square root of MSE\n",
    "\n",
    "        # Convert to binary labels (using TensorFlow)\n",
    "        binary_actual = tf.cast(y_test > threshold, tf.float32)\n",
    "        binary_pred = tf.cast(predictions > threshold, tf.float32)\n",
    "\n",
    "        # Calculate True Positives, False Positives, False Negatives, True Negatives\n",
    "        tp = tf.reduce_sum(binary_actual * binary_pred)\n",
    "        fp = tf.reduce_sum((1 - binary_actual) * binary_pred)\n",
    "        fn = tf.reduce_sum(binary_actual * (1 - binary_pred))\n",
    "        tn = tf.reduce_sum((1 - binary_actual) * (1 - binary_pred))\n",
    "\n",
    "        # Handle division by zero\n",
    "        epsilon = 1e-7  # Small constant to prevent division by zero\n",
    "\n",
    "        # Compute additional metrics (using TensorFlow)\n",
    "        csi = tp / (tp + fn + fp + epsilon)\n",
    "        pod = tp / (tp + fn + epsilon)\n",
    "        far = fp / (tp + fp + epsilon)\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)  # This is the same as POD\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "        # Compute false negative rate (using TensorFlow)\n",
    "        total_positives = tf.reduce_sum(tf.cast(y_test > threshold, tf.float32))\n",
    "        false_negatives = tf.cast(fn, tf.float32) #Cast to float32 for division\n",
    "        false_negative_rate = false_negatives / (total_positives + epsilon)\n",
    "\n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'false_negative_rate': false_negative_rate,\n",
    "            'CSI': csi,\n",
    "            'POD': pod,\n",
    "            'FAR': far,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise #Re-raise the exception\n",
    "\n",
    "def plot_predictions(model, X_data, y_data, label, start=50, end=500, ylabel='Rainfall (mm)', title_suffix=''):\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_data).flatten()\n",
    "\n",
    "    # Create a DataFrame to store results\n",
    "    results_df = pd.DataFrame(data={f'{label} Predictions': predictions, 'Actual Values': y_data})\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the predictions and actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[f'{label} Predictions'][start:end], label=f'{label} Predictions', color='blue', linestyle='-')\n",
    "    plt.plot(results_df['Actual Values'][start:end], label='Actual Values', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time Stamps', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(f'{label} Predictions vs Actual Values {title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********DATA INGESTION COMPLETE********\n",
      "********DATA PREPROCESSING COMPLETE********\n",
      "\n",
      "Training LSTM model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 33ms/step - loss: 84.6689 - mae: 5.1452 - mse: 49.7539 - root_mean_squared_error: 6.7065 - val_loss: 2.1523 - val_mae: 1.0678 - val_mse: 1.8004 - val_root_mean_squared_error: 1.3418 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 31ms/step - loss: 15.3832 - mae: 2.5734 - mse: 10.9772 - root_mean_squared_error: 3.3123 - val_loss: 3.2571 - val_mae: 1.0293 - val_mse: 1.6670 - val_root_mean_squared_error: 1.2911 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 31ms/step - loss: 12.1311 - mae: 2.2966 - mse: 8.6675 - root_mean_squared_error: 2.9435 - val_loss: 2.0654 - val_mae: 0.9552 - val_mse: 1.4506 - val_root_mean_squared_error: 1.2044 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 33ms/step - loss: 9.5190 - mae: 2.0057 - mse: 6.7579 - root_mean_squared_error: 2.5988 - val_loss: 1.7326 - val_mae: 0.8311 - val_mse: 1.2366 - val_root_mean_squared_error: 1.1120 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 34ms/step - loss: 7.5576 - mae: 1.7883 - mse: 5.3993 - root_mean_squared_error: 2.3236 - val_loss: 1.5342 - val_mae: 0.6888 - val_mse: 0.9059 - val_root_mean_squared_error: 0.9518 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 10ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     7.648808            4.9\n",
      "1                     7.648808            4.8\n",
      "2                     7.648808            4.7\n",
      "3                     7.703138            7.1\n",
      "4                    13.441164           13.7\n",
      "...                        ...            ...\n",
      "175329               17.724932           17.7\n",
      "175330               17.182438           16.5\n",
      "175331               15.866030           15.2\n",
      "175332               14.297531           15.1\n",
      "175333               14.003089           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 10.360707           10.9\n",
      "1                  9.761010            9.4\n",
      "2                  8.143935            8.2\n",
      "3                  7.648808            8.1\n",
      "4                  7.648808            7.8\n",
      "...                     ...            ...\n",
      "21911             28.325483           29.0\n",
      "21912             27.446892           27.0\n",
      "21913             25.591892           26.4\n",
      "21914             24.876884           25.6\n",
      "21915             24.370163           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  24.575455           25.4\n",
      "1                  24.417442           25.3\n",
      "2                  24.522156           24.6\n",
      "3                  23.758984           24.4\n",
      "4                  23.577835           24.8\n",
      "...                      ...            ...\n",
      "21913               7.648808            9.8\n",
      "21914               7.932643           10.6\n",
      "21915               9.048547           11.1\n",
      "21916               9.811117           10.5\n",
      "21917               8.960665            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 31ms/step - loss: 77.9466 - mae: 4.9409 - mse: 46.1146 - root_mean_squared_error: 6.4694 - val_loss: 2.1662 - val_mae: 0.8864 - val_mse: 1.4584 - val_root_mean_squared_error: 1.2076 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 32ms/step - loss: 14.5288 - mae: 2.5077 - mse: 10.4495 - root_mean_squared_error: 3.2315 - val_loss: 1.8465 - val_mae: 0.9112 - val_mse: 1.5221 - val_root_mean_squared_error: 1.2337 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - loss: 10.7925 - mae: 2.1453 - mse: 7.7648 - root_mean_squared_error: 2.7859 - val_loss: 1.7435 - val_mae: 0.9338 - val_mse: 1.4891 - val_root_mean_squared_error: 1.2203 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 36ms/step - loss: 8.2649 - mae: 1.8797 - mse: 5.9768 - root_mean_squared_error: 2.4445 - val_loss: 1.2569 - val_mae: 0.6999 - val_mse: 0.9163 - val_root_mean_squared_error: 0.9572 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 37ms/step - loss: 7.0926 - mae: 1.7492 - mse: 5.1922 - root_mean_squared_error: 2.2786 - val_loss: 1.4502 - val_mae: 0.7931 - val_mse: 1.1356 - val_root_mean_squared_error: 1.0657 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    6.875553            4.9\n",
      "1                    6.875553            4.8\n",
      "2                    6.875553            4.7\n",
      "3                    6.875553            7.1\n",
      "4                   13.369440           13.7\n",
      "...                       ...            ...\n",
      "175329              17.579660           17.7\n",
      "175330              16.770697           16.5\n",
      "175331              15.272876           15.2\n",
      "175332              13.851348           15.1\n",
      "175333              14.586603           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.981913           10.9\n",
      "1                10.740870            9.4\n",
      "2                 8.584433            8.2\n",
      "3                 7.281895            8.1\n",
      "4                 7.141566            7.8\n",
      "...                    ...            ...\n",
      "21911            28.816122           29.0\n",
      "21912            27.485523           27.0\n",
      "21913            26.082722           26.4\n",
      "21914            25.452824           25.6\n",
      "21915            25.161961           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.101917           25.4\n",
      "1                 24.978336           25.3\n",
      "2                 24.931709           24.6\n",
      "3                 24.157642           24.4\n",
      "4                 23.951756           24.8\n",
      "...                     ...            ...\n",
      "21913              7.908050            9.8\n",
      "21914              8.845997           10.6\n",
      "21915              9.551224           11.1\n",
      "21916              9.890918           10.5\n",
      "21917              9.417376            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN_LSTM model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 19ms/step - loss: 55.0622 - mae: 3.9853 - mse: 32.8419 - root_mean_squared_error: 5.3753 - val_loss: 2.5741 - val_mae: 1.0759 - val_mse: 2.0100 - val_root_mean_squared_error: 1.4178 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 18ms/step - loss: 16.3354 - mae: 2.6004 - mse: 11.4970 - root_mean_squared_error: 3.3900 - val_loss: 2.0004 - val_mae: 0.9667 - val_mse: 1.6026 - val_root_mean_squared_error: 1.2660 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 18ms/step - loss: 12.4851 - mae: 2.2456 - mse: 8.7651 - root_mean_squared_error: 2.9601 - val_loss: 1.5908 - val_mae: 0.8012 - val_mse: 1.1240 - val_root_mean_squared_error: 1.0602 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 18ms/step - loss: 9.6238 - mae: 1.9548 - mse: 6.7601 - root_mean_squared_error: 2.5996 - val_loss: 1.5828 - val_mae: 0.7904 - val_mse: 1.1163 - val_root_mean_squared_error: 1.0565 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 18ms/step - loss: 7.5137 - mae: 1.7213 - mse: 5.2969 - root_mean_squared_error: 2.3011 - val_loss: 1.3582 - val_mae: 0.7448 - val_mse: 1.0343 - val_root_mean_squared_error: 1.0170 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step\n",
      "        cnn_lstm Train Predictions  Actual Values\n",
      "0                         4.763993            4.9\n",
      "1                         4.668388            4.8\n",
      "2                         4.597516            4.7\n",
      "3                         6.720726            7.1\n",
      "4                        12.903440           13.7\n",
      "...                            ...            ...\n",
      "175329                   17.377586           17.7\n",
      "175330                   16.561432           16.5\n",
      "175331                   15.143337           15.2\n",
      "175332                   13.480831           15.1\n",
      "175333                   13.875800           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
      "       cnn_lstm Val Predictions  Actual Values\n",
      "0                     10.563423           10.9\n",
      "1                     10.097066            9.4\n",
      "2                      8.711798            8.2\n",
      "3                      7.770401            8.1\n",
      "4                      7.491483            7.8\n",
      "...                         ...            ...\n",
      "21911                 28.243700           29.0\n",
      "21912                 27.641342           27.0\n",
      "21913                 26.721495           26.4\n",
      "21914                 26.066395           25.6\n",
      "21915                 25.567654           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
      "       cnn_lstm Test Predictions  Actual Values\n",
      "0                      25.165604           25.4\n",
      "1                      24.728161           25.3\n",
      "2                      24.769476           24.6\n",
      "3                      24.515394           24.4\n",
      "4                      24.432877           24.8\n",
      "...                          ...            ...\n",
      "21913                   8.420531            9.8\n",
      "21914                   9.561892           10.6\n",
      "21915                  10.089546           11.1\n",
      "21916                  10.321569           10.5\n",
      "21917                   9.897095            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'weighted_mse' saved in '../artifacts/results/cycle_2/test_4/adam_weighted_mse'.\n",
      "\n",
      "Training LSTM model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 32ms/step - loss: 84.1806 - mae: 5.1370 - mse: 49.7483 - root_mean_squared_error: 6.7220 - val_loss: 2.1730 - val_mae: 1.0317 - val_mse: 1.6974 - val_root_mean_squared_error: 1.3028 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 32ms/step - loss: 15.3137 - mae: 2.5524 - mse: 10.8901 - root_mean_squared_error: 3.2969 - val_loss: 1.9188 - val_mae: 0.9183 - val_mse: 1.3918 - val_root_mean_squared_error: 1.1798 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 33ms/step - loss: 9.9168 - mae: 2.0389 - mse: 7.0681 - root_mean_squared_error: 2.6579 - val_loss: 1.8291 - val_mae: 1.0229 - val_mse: 1.5568 - val_root_mean_squared_error: 1.2477 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 33ms/step - loss: 7.7418 - mae: 1.8034 - mse: 5.5253 - root_mean_squared_error: 2.3504 - val_loss: 1.3892 - val_mae: 0.7811 - val_mse: 1.0852 - val_root_mean_squared_error: 1.0417 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 32ms/step - loss: 6.8552 - mae: 1.7064 - mse: 4.9575 - root_mean_squared_error: 2.2264 - val_loss: 1.9384 - val_mae: 0.9474 - val_mse: 1.5252 - val_root_mean_squared_error: 1.2350 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 19ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     6.966909            4.9\n",
      "1                     6.966909            4.8\n",
      "2                     6.966909            4.7\n",
      "3                     6.990376            7.1\n",
      "4                    14.566813           13.7\n",
      "...                        ...            ...\n",
      "175329               18.271702           17.7\n",
      "175330               17.666958           16.5\n",
      "175331               16.443102           15.2\n",
      "175332               14.488830           15.1\n",
      "175333               14.397276           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 11.249151           10.9\n",
      "1                 10.636032            9.4\n",
      "2                  9.378252            8.2\n",
      "3                  8.171596            8.1\n",
      "4                  7.875155            7.8\n",
      "...                     ...            ...\n",
      "21911             28.868860           29.0\n",
      "21912             28.009216           27.0\n",
      "21913             26.395555           26.4\n",
      "21914             25.974512           25.6\n",
      "21915             25.590223           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  25.557669           25.4\n",
      "1                  25.123894           25.3\n",
      "2                  25.116619           24.6\n",
      "3                  24.554163           24.4\n",
      "4                  24.323713           24.8\n",
      "...                      ...            ...\n",
      "21913               8.000847            9.8\n",
      "21914               9.003841           10.6\n",
      "21915               9.924708           11.1\n",
      "21916              10.413183           10.5\n",
      "21917               9.863678            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 56ms/step - loss: 83.5324 - mae: 5.0981 - mse: 49.0558 - root_mean_squared_error: 6.6586 - val_loss: 2.3417 - val_mae: 1.0084 - val_mse: 1.7598 - val_root_mean_squared_error: 1.3266 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 46ms/step - loss: 14.5758 - mae: 2.5165 - mse: 10.4870 - root_mean_squared_error: 3.2367 - val_loss: 1.6495 - val_mae: 0.9030 - val_mse: 1.3274 - val_root_mean_squared_error: 1.1521 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 46ms/step - loss: 10.3628 - mae: 2.1055 - mse: 7.4702 - root_mean_squared_error: 2.7325 - val_loss: 1.4881 - val_mae: 0.8437 - val_mse: 1.2279 - val_root_mean_squared_error: 1.1081 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 47ms/step - loss: 8.1611 - mae: 1.8648 - mse: 5.9028 - root_mean_squared_error: 2.4293 - val_loss: 1.5079 - val_mae: 0.8218 - val_mse: 1.2243 - val_root_mean_squared_error: 1.1065 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 49ms/step - loss: 7.0797 - mae: 1.7520 - mse: 5.2121 - root_mean_squared_error: 2.2822 - val_loss: 1.5061 - val_mae: 0.8058 - val_mse: 1.1865 - val_root_mean_squared_error: 1.0893 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 13ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    7.872081            4.9\n",
      "1                    7.872081            4.8\n",
      "2                    7.872081            4.7\n",
      "3                    7.872081            7.1\n",
      "4                   14.328093           13.7\n",
      "...                       ...            ...\n",
      "175329              18.438433           17.7\n",
      "175330              17.817333           16.5\n",
      "175331              16.082676           15.2\n",
      "175332              14.481240           15.1\n",
      "175333              14.750753           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.969975           10.9\n",
      "1                11.008135            9.4\n",
      "2                 9.089870            8.2\n",
      "3                 8.189610            8.1\n",
      "4                 7.872081            7.8\n",
      "...                    ...            ...\n",
      "21911            28.212492           29.0\n",
      "21912            27.431881           27.0\n",
      "21913            25.957970           26.4\n",
      "21914            25.562561           25.6\n",
      "21915            25.063826           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.129034           25.4\n",
      "1                 24.974442           25.3\n",
      "2                 24.908804           24.6\n",
      "3                 24.084869           24.4\n",
      "4                 24.174307           24.8\n",
      "...                     ...            ...\n",
      "21913              7.872081            9.8\n",
      "21914              8.127401           10.6\n",
      "21915              8.912825           11.1\n",
      "21916              9.601057           10.5\n",
      "21917              9.261895            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN_LSTM model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 49ms/step - loss: 63.7445 - mae: 4.3663 - mse: 38.0856 - root_mean_squared_error: 5.8403 - val_loss: 2.7811 - val_mae: 1.1978 - val_mse: 2.3128 - val_root_mean_squared_error: 1.5208 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 47ms/step - loss: 22.3759 - mae: 2.9912 - mse: 15.3449 - root_mean_squared_error: 3.9164 - val_loss: 2.1430 - val_mae: 0.9265 - val_mse: 1.5300 - val_root_mean_squared_error: 1.2369 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 47ms/step - loss: 16.2112 - mae: 2.5069 - mse: 11.1384 - root_mean_squared_error: 3.3367 - val_loss: 1.8150 - val_mae: 0.8107 - val_mse: 1.1719 - val_root_mean_squared_error: 1.0826 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 47ms/step - loss: 11.5543 - mae: 2.1005 - mse: 7.9654 - root_mean_squared_error: 2.8217 - val_loss: 1.6875 - val_mae: 0.9402 - val_mse: 1.4437 - val_root_mean_squared_error: 1.2015 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 47ms/step - loss: 8.4720 - mae: 1.7838 - mse: 5.8383 - root_mean_squared_error: 2.4157 - val_loss: 1.7073 - val_mae: 0.8129 - val_mse: 1.2104 - val_root_mean_squared_error: 1.1002 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 10ms/step\n",
      "        cnn_lstm Train Predictions  Actual Values\n",
      "0                         4.358042            4.9\n",
      "1                         4.233703            4.8\n",
      "2                         4.234093            4.7\n",
      "3                         6.465403            7.1\n",
      "4                        13.536676           13.7\n",
      "...                            ...            ...\n",
      "175329                   18.300880           17.7\n",
      "175330                   17.564953           16.5\n",
      "175331                   16.066807           15.2\n",
      "175332                   14.137872           15.1\n",
      "175333                   14.081301           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step\n",
      "       cnn_lstm Val Predictions  Actual Values\n",
      "0                     11.584329           10.9\n",
      "1                     10.897532            9.4\n",
      "2                      9.758248            8.2\n",
      "3                      8.037626            8.1\n",
      "4                      7.616786            7.8\n",
      "...                         ...            ...\n",
      "21911                 28.746964           29.0\n",
      "21912                 27.964363           27.0\n",
      "21913                 26.726334           26.4\n",
      "21914                 25.940947           25.6\n",
      "21915                 25.523052           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n",
      "       cnn_lstm Test Predictions  Actual Values\n",
      "0                      25.325176           25.4\n",
      "1                      24.878387           25.3\n",
      "2                      24.626728           24.6\n",
      "3                      24.621281           24.4\n",
      "4                      24.370438           24.8\n",
      "...                          ...            ...\n",
      "21913                   9.119854            9.8\n",
      "21914                   9.928309           10.6\n",
      "21915                  10.456866           11.1\n",
      "21916                  10.647385           10.5\n",
      "21917                  10.304960            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'weighted_mse' saved in '../artifacts/results/cycle_2/test_4/adamw_weighted_mse'.\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "LSTM:\n",
      "mse: 1.0659\n",
      "mae: 0.7721\n",
      "rmse: 1.0324\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "GRU:\n",
      "mse: 1.1256\n",
      "mae: 0.7887\n",
      "rmse: 1.0609\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "CNN_LSTM:\n",
      "mse: 1.3704\n",
      "mae: 0.9165\n",
      "rmse: 1.1706\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv('../artifacts/dataset/01-hourly_historical_analyzed_data.csv')\n",
    "        df = df.drop(columns=['hour', 'day', 'month', 'year'])\n",
    "        \n",
    "        # # testing 0.1\n",
    "        # print(f\"df Dataframe: {df.head()}\")\n",
    "        \n",
    "        # Convert to datetime index\n",
    "        df1 = df.copy()\n",
    "        df1.index = pd.to_datetime(df1['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # # testing 0.2\n",
    "        # print(f\"df1 Dataframe: {df1.head()}\")\n",
    "        \n",
    "        print(\"********DATA INGESTION COMPLETE********\")\n",
    "        \n",
    "        # Extract rain data\n",
    "        rain = df1['rain']\n",
    "        rain_df = pd.DataFrame(rain)\n",
    "        \n",
    "        # Preprocess data with enhanced features\n",
    "        rain_df = preprocess_data(rain_df)\n",
    "        rain_df = rain_df.drop(['rain'], axis=1)\n",
    "        \n",
    "        # # testing 0.3\n",
    "        # print(f\"rain_df Dataframe: {rain_df.head()}\")\n",
    "        \n",
    "        processed_df = pd.concat([df1, rain_df], axis=1)\n",
    "        processed_df = processed_df.drop(['time'], axis=1)\n",
    "        \n",
    "        # # testing 0.4\n",
    "        # print(f\"processed_df Dataframe: {processed_df.head()}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = df_to_X_y(processed_df, window_size=24)\n",
    "        \n",
    "        # # testing 0.5\n",
    "        # print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_time_series_data(X, y)\n",
    "        \n",
    "        # # testing 0.6\n",
    "        # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        # print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        # Scale the data\n",
    "        X_train, X_val, X_test, X_scaler = standardize_features(X_train, X_val, X_test)\n",
    "        \n",
    "        # # testing 0.7\n",
    "        # print(\"Standardization complete\")\n",
    "        \n",
    "        print(\"********DATA PREPROCESSING COMPLETE********\")\n",
    "        \n",
    "        # Define optimizers and loss functions to iterate over\n",
    "        optimizers = ['adam', 'adamw']\n",
    "        # loss_functions = ['weighted_mse', 'custom']\n",
    "        loss_functions = ['weighted_mse']\n",
    "        \n",
    "        # for loss_function in loss_functions:\n",
    "        \n",
    "        #     # Create a directory for the current loss function\n",
    "        #     results_dir = f'../artifacts/results/cycle_2/test_3/{loss_function}'\n",
    "        #     os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        #     # Train models\n",
    "        #     models = {\n",
    "        #         'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #         'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #         'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #     }\n",
    "            \n",
    "        #     results = {}\n",
    "        #     for name, model in models.items():\n",
    "        #         print(f\"\\nTraining {name.upper()} model with {loss_function.upper()} loss...\")\n",
    "                \n",
    "        #         try:\n",
    "        #             history = train_model(\n",
    "        #                 model, X_train, y_train, X_val, y_val,\n",
    "        #                 f'../artifacts/models/cycle_2/test_3/model_{name}_{loss_function}.keras', \n",
    "        #                 epochs=5, loss_function=loss_function, learning_rate=1e-3\n",
    "        #             )\n",
    "                    \n",
    "        #             print(\"********MODEL TRAINING COMPLETE********\")\n",
    "        #         except Exception as e:\n",
    "        #             print(f\"Error training {name.upper()} model: {e}\")\n",
    "        #             continue\n",
    "        \n",
    "        for optimizer_function in optimizers:\n",
    "            for loss_function in loss_functions:\n",
    "                \n",
    "                # Create a directory for the current optimizer-loss function combination\n",
    "                results_dir = f'../artifacts/results/cycle_2/test_5/{optimizer_function}_{loss_function}'\n",
    "                os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "                # Train models\n",
    "                models = {\n",
    "                    'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    # 'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    # 'cnn_lstm': create_cnn_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                }\n",
    "\n",
    "                results = {}\n",
    "                for name, model in models.items():\n",
    "                    print(f\"\\nTraining {name.upper()} model with {optimizer_function.upper()} optimizer and {loss_function.upper()} loss...\")\n",
    "\n",
    "                    try:\n",
    "                        history = train_model(\n",
    "                            model, X_train, y_train, X_val, y_val,\n",
    "                            f'../artifacts/models/cycle_2/test_5/model_{name}_{optimizer_function}_{loss_function}.keras', \n",
    "                            epochs=10, loss_function=loss_function, optimizer_function=optimizer_function, learning_rate=1e-3\n",
    "                        )\n",
    "\n",
    "                        print(\"********MODEL TRAINING COMPLETE********\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error training {name.upper()} model: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                    try:\n",
    "                        results[name] = evaluate_model(model, X_test, y_test)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error evaluating {name.upper()} model: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Save training history to CSV\n",
    "                    history_df = pd.DataFrame(history.history)\n",
    "                    history_df.to_csv(os.path.join(results_dir, f'{name}_history.csv'), index=False)\n",
    "                    \n",
    "                    # Plot training history\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.plot(history.history['loss'], label='Training Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.title(f'{name.upper()} Model Training History')\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.savefig(os.path.join(results_dir, f'{name}_training_history.png'))\n",
    "                    # plt.show()\n",
    "                    plt.close()  # Close the plot to free memory\n",
    "                    \n",
    "                    # Save evaluation results to a text file\n",
    "                    with open(os.path.join(results_dir, f'{name}_evaluation.txt'), 'w') as f:\n",
    "                        for metric_name, value in results[name].items():\n",
    "                            f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "                    \n",
    "                    # Plot predictions for Train, Val, and Test datasets and save the plots\n",
    "                    for dataset, data, true_values in zip(['Train', 'Val', 'Test'], \n",
    "                                                        [X_train, X_val, X_test], \n",
    "                                                        [y_train, y_val, y_test]):\n",
    "                        plot_predictions(\n",
    "                            model=model, \n",
    "                            X_data=data, \n",
    "                            y_data=true_values, \n",
    "                            label=name + ' ' + dataset, \n",
    "                            start=100, \n",
    "                            end=500\n",
    "                        )\n",
    "                        plt.savefig(os.path.join(results_dir, f'{name}_{dataset.lower()}_predictions.png'))\n",
    "                        # plt.show()\n",
    "                        plt.close()  # Close the plot to free memory\n",
    "                        \n",
    "                    print(\"********MODEL EVALUATION COMPLETE********\")\n",
    "            \n",
    "            print(f\"Results for loss function '{loss_function}' saved in '{results_dir}'.\")\n",
    "            \n",
    "        # Return results\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled error in execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best performance now\n",
    "1. GRU with *weighted loss* **(cycle_2/test_1)**\n",
    "   * mse: 0.8157\n",
    "   * mae: 0.6507\n",
    "   * false_negative_rate: 0.0000\n",
    "2. LSTM with *weighted loss* with *AdamW optimizer* **(cycle_2/test_3)**\n",
    "   * mse: 0.7435\n",
    "   * mae: 0.6114\n",
    "   * rmse: 0.8623\n",
    "   * false_negative_rate: 0.0000\n",
    "3. LSTM with *weighted loss* with *AdamW optimizer* **(cycle_2/test_5)**\n",
    "   * mse: 0.7347\n",
    "   * mae: 0.6035\n",
    "   * rmse: 0.8571\n",
    "   * flse_negative_rate: 0.0000\n",
    "\n",
    "## overall best performance till now\n",
    "1. LSTM with *weighted loss* **(cycle_1/test_14)**\n",
    "   * mse: 0.7585\n",
    "   * mae: 0.6146\n",
    "   * false_negative_rate: 0.0000\n",
    "\n",
    "## All models have zero false negative rate: \n",
    "This means none of the models failed to predict rainfall events when they actually occurred, which is particularly important for weather forecasting.\n",
    "\n",
    "## Low values: \n",
    "All the error metrics are relatively low, suggesting that all three models performed reasonably well.\n",
    "\n",
    "## Training curves: \n",
    "All models show good convergence in their training histories, with both training and validation loss decreasing and stabilizing, indicating proper training without significant overfitting.\n",
    "\n",
    "## Visual patterns: \n",
    "From the graphs, all models seem to follow the general pattern of the actual rainfall values, but they struggle with extreme values (particularly low rainfall values where the actual data shows near-zero measurements)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
