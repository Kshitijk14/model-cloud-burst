{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    try:\n",
    "        # Process temporal features\n",
    "        df['Seconds'] = df.index.map(pd.Timestamp.timestamp)\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = (365.2425) * day\n",
    "        \n",
    "        # Add cyclical time features\n",
    "        df['Day sin'] = np.sin(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Day cos'] = np.cos(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Year sin'] = np.sin(df['Seconds'] * (2 * np.pi / year))\n",
    "        df['Year cos'] = np.cos(df['Seconds'] * (2 * np.pi / year))\n",
    "        \n",
    "        # # Add rolling statistics\n",
    "        # df['rolling_mean_6h'] = df.iloc[:, 0].rolling(window=6).mean()\n",
    "        # df['rolling_std_6h'] = df.iloc[:, 0].rolling(window=6).std()\n",
    "        # df['rolling_max_6h'] = df.iloc[:, 0].rolling(window=6).max()\n",
    "        \n",
    "        # # Add lag features\n",
    "        # for i in [1, 3, 6, 12]:\n",
    "        #     df[f'lag_{i}'] = df.iloc[:, 0].shift(i)\n",
    "        \n",
    "        df = df.bfill()\n",
    "        df = df.drop(['Seconds'], axis=1)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def df_to_X_y(df, window_size=24):\n",
    "    try:\n",
    "        df_as_np = df.to_numpy()\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(len(df_as_np) - window_size):\n",
    "            row = [r for r in df_as_np[i: i + window_size]]\n",
    "            X.append(row)\n",
    "            \n",
    "            label = df_as_np[i + window_size][0]\n",
    "            y.append(label)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in df_to_X_y: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_time_series_data(X, y, train_ratio=0.8, val_ratio=0.1):\n",
    "    try:\n",
    "        n = len(X)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "        X_train, y_train = X[:train_end], y[:train_end]\n",
    "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "        X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in split_time_series_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def standardize_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Standardize all features in the dataset while maintaining the 3D structure\n",
    "    (samples, timesteps, features).\n",
    "    \"\"\"\n",
    "    num_samples, num_timesteps, num_features = X_train.shape\n",
    "    \n",
    "    # Reshape to 2D for StandardScaler (combine samples & timesteps)\n",
    "    X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "    \n",
    "    # Fit the scaler on training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "    # Transform validation and test sets\n",
    "    X_val_scaled = scaler.transform(X_val.reshape(-1, num_features))\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, num_features))\n",
    "\n",
    "    # Reshape back to original 3D shape\n",
    "    X_train = X_train_scaled.reshape(num_samples, num_timesteps, num_features)\n",
    "    X_val = X_val_scaled.reshape(X_val.shape[0], X_val.shape[1], num_features)\n",
    "    X_test = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], num_features)\n",
    "\n",
    "    return X_train, X_val, X_test, scaler\n",
    "\n",
    "# Model architectures with regularization and dropout\n",
    "def create_lstm_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            LSTM(\n",
    "                128, \n",
    "                return_sequences=True, \n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            LSTM(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_lstm_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_gru_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            GRU(\n",
    "                128, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            GRU(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_gru_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_cnn_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            Conv1D(\n",
    "                filters=128, \n",
    "                kernel_size=3, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Conv1D(\n",
    "                filters=64, \n",
    "                kernel_size=3, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            Flatten(),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_cnn_model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Weighted mean squared error\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    try:\n",
    "        weights = tf.where(y_pred < y_true, 2.0, 1.0) # Penalize underestimation more heavily (false negatives)\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        return tf.reduce_mean(weights * squared_difference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in weighted_mse: {e}\")\n",
    "        raise\n",
    "\n",
    "# Custom loss function to penalize false negatives more heavily\n",
    "def custom_loss(y_true, y_pred):\n",
    "    try:\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        mse = tf.reduce_mean(squared_difference, axis=-1)\n",
    "        \n",
    "        # Penalize underestimation more heavily (false negatives)\n",
    "        penalty = tf.where(y_pred < y_true, 2.0, 1.0)\n",
    "        return mse * penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_loss: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Training function with callbacks\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_path, \n",
    "                batch_size=32, epochs=5, patience=15, loss_function='mse', learning_rate=1e-3):\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
    "            ModelCheckpoint(model_path, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Select the loss function based on the input parameter\n",
    "        if loss_function == 'weighted_mse':\n",
    "            loss = weighted_mse\n",
    "        elif loss_function == 'custom':\n",
    "            loss = custom_loss\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                'mse', \n",
    "                'mae',\n",
    "                'root_mean_squared_error',\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Evaluation function with focus on false negatives\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Compute MSE and MAE\n",
    "        mse = np.mean((y_test - predictions.flatten())**2)\n",
    "        mae = np.mean(np.abs(y_test - predictions.flatten()))\n",
    "        \n",
    "        # Convert to binary labels\n",
    "        binary_actual = y_test > threshold\n",
    "        binary_pred = predictions.flatten() > threshold\n",
    "        \n",
    "        # Compute false negatives and FNR\n",
    "        false_negatives = np.sum((binary_actual == True) & (binary_pred == False))\n",
    "        total_positives = np.sum(binary_actual)\n",
    "        false_negative_rate = false_negatives / total_positives if total_positives > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'false_negative_rate': false_negative_rate\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_predictions(model, X_data, y_data, label, start=50, end=500, ylabel='Rainfall (mm)', title_suffix=''):\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_data).flatten()\n",
    "\n",
    "    # Create a DataFrame to store results\n",
    "    results_df = pd.DataFrame(data={f'{label} Predictions': predictions, 'Actual Values': y_data})\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the predictions and actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[f'{label} Predictions'][start:end], label=f'{label} Predictions', color='blue', linestyle='-')\n",
    "    plt.plot(results_df['Actual Values'][start:end], label='Actual Values', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time Stamps', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(f'{label} Predictions vs Actual Values {title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********DATA INGESTION COMPLETE********\n",
      "********DATA PREPROCESSING COMPLETE********\n",
      "\n",
      "Training LSTM model with WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 35ms/step - loss: 86.6114 - mae: 5.3419 - mse: 52.0980 - root_mean_squared_error: 6.9509 - val_loss: 2.6358 - val_mae: 0.9992 - val_mse: 1.7224 - val_root_mean_squared_error: 1.3124 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 67ms/step - loss: 19.9454 - mae: 2.9130 - mse: 14.1350 - root_mean_squared_error: 3.7584 - val_loss: 1.9126 - val_mae: 0.9487 - val_mse: 1.4653 - val_root_mean_squared_error: 1.2105 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 43ms/step - loss: 13.1989 - mae: 2.3833 - mse: 9.4346 - root_mean_squared_error: 3.0710 - val_loss: 2.1787 - val_mae: 1.0173 - val_mse: 1.6968 - val_root_mean_squared_error: 1.3026 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 33ms/step - loss: 10.4463 - mae: 2.1290 - mse: 7.4791 - root_mean_squared_error: 2.7343 - val_loss: 1.7193 - val_mae: 0.9057 - val_mse: 1.3425 - val_root_mean_squared_error: 1.1587 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 34ms/step - loss: 8.3050 - mae: 1.8735 - mse: 5.8908 - root_mean_squared_error: 2.4265 - val_loss: 1.7204 - val_mae: 0.8810 - val_mse: 1.2981 - val_root_mean_squared_error: 1.1393 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 10ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     4.402318            4.9\n",
      "1                     4.337075            4.8\n",
      "2                     4.458785            4.7\n",
      "3                     5.877053            7.1\n",
      "4                    14.108580           13.7\n",
      "...                        ...            ...\n",
      "175329               18.414240           17.7\n",
      "175330               17.790112           16.5\n",
      "175331               15.968950           15.2\n",
      "175332               13.865663           15.1\n",
      "175333               13.545539           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 10.540703           10.9\n",
      "1                 10.801019            9.4\n",
      "2                  9.219927            8.2\n",
      "3                  8.155832            8.1\n",
      "4                  7.365460            7.8\n",
      "...                     ...            ...\n",
      "21911             29.434721           29.0\n",
      "21912             28.342772           27.0\n",
      "21913             26.662836           26.4\n",
      "21914             26.080536           25.6\n",
      "21915             25.609465           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  25.880037           25.4\n",
      "1                  25.401522           25.3\n",
      "2                  25.174463           24.6\n",
      "3                  24.666815           24.4\n",
      "4                  24.633867           24.8\n",
      "...                      ...            ...\n",
      "21913               7.534471            9.8\n",
      "21914               8.024096           10.6\n",
      "21915               9.004717           11.1\n",
      "21916              10.092553           10.5\n",
      "21917               9.592660            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 32ms/step - loss: 75.7966 - mae: 4.8829 - mse: 44.6075 - root_mean_squared_error: 6.3477 - val_loss: 3.2523 - val_mae: 1.0867 - val_mse: 1.8108 - val_root_mean_squared_error: 1.3456 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 32ms/step - loss: 14.5449 - mae: 2.5024 - mse: 10.4425 - root_mean_squared_error: 3.2303 - val_loss: 1.8604 - val_mae: 0.8917 - val_mse: 1.3132 - val_root_mean_squared_error: 1.1460 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - loss: 10.2965 - mae: 2.0952 - mse: 7.4140 - root_mean_squared_error: 2.7221 - val_loss: 1.5431 - val_mae: 0.8954 - val_mse: 1.3256 - val_root_mean_squared_error: 1.1513 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 32ms/step - loss: 8.0437 - mae: 1.8558 - mse: 5.8334 - root_mean_squared_error: 2.4150 - val_loss: 1.1998 - val_mae: 0.6655 - val_mse: 0.8522 - val_root_mean_squared_error: 0.9231 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 33ms/step - loss: 7.0064 - mae: 1.7437 - mse: 5.1506 - root_mean_squared_error: 2.2693 - val_loss: 1.6162 - val_mae: 0.8882 - val_mse: 1.3317 - val_root_mean_squared_error: 1.1540 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 8ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    6.946453            4.9\n",
      "1                    6.946453            4.8\n",
      "2                    6.946453            4.7\n",
      "3                    6.946453            7.1\n",
      "4                   12.851892           13.7\n",
      "...                       ...            ...\n",
      "175329              17.638475           17.7\n",
      "175330              17.040836           16.5\n",
      "175331              15.819839           15.2\n",
      "175332              14.335076           15.1\n",
      "175333              13.993274           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.885066           10.9\n",
      "1                 9.959726            9.4\n",
      "2                 8.381029            8.2\n",
      "3                 7.260043            8.1\n",
      "4                 7.181417            7.8\n",
      "...                    ...            ...\n",
      "21911            28.720306           29.0\n",
      "21912            28.016407           27.0\n",
      "21913            26.433447           26.4\n",
      "21914            25.805321           25.6\n",
      "21915            25.269440           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.084221           25.4\n",
      "1                 24.623343           25.3\n",
      "2                 24.749750           24.6\n",
      "3                 24.379183           24.4\n",
      "4                 24.045269           24.8\n",
      "...                     ...            ...\n",
      "21913              8.024390            9.8\n",
      "21914              8.973223           10.6\n",
      "21915              9.876575           11.1\n",
      "21916             10.313387           10.5\n",
      "21917              9.818391            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 10ms/step - loss: 57.9751 - mae: 4.3158 - mse: 35.6796 - root_mean_squared_error: 5.7053 - val_loss: 5.0381 - val_mae: 1.2954 - val_mse: 2.6077 - val_root_mean_squared_error: 1.6148 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 12ms/step - loss: 19.6936 - mae: 2.8649 - mse: 13.8877 - root_mean_squared_error: 3.7257 - val_loss: 2.0739 - val_mae: 0.9920 - val_mse: 1.6464 - val_root_mean_squared_error: 1.2831 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 9ms/step - loss: 15.4045 - mae: 2.5151 - mse: 10.8745 - root_mean_squared_error: 3.2971 - val_loss: 1.9950 - val_mae: 0.8315 - val_mse: 1.1626 - val_root_mean_squared_error: 1.0783 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 8ms/step - loss: 12.1216 - mae: 2.2103 - mse: 8.5264 - root_mean_squared_error: 2.9193 - val_loss: 1.5496 - val_mae: 0.7661 - val_mse: 1.0528 - val_root_mean_squared_error: 1.0261 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 8ms/step - loss: 9.4620 - mae: 1.9424 - mse: 6.6587 - root_mean_squared_error: 2.5801 - val_loss: 1.3144 - val_mae: 0.6944 - val_mse: 0.8826 - val_root_mean_squared_error: 0.9395 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    5.083338            4.9\n",
      "1                    5.363160            4.8\n",
      "2                    5.888197            4.7\n",
      "3                    7.642280            7.1\n",
      "4                   13.576577           13.7\n",
      "...                       ...            ...\n",
      "175329              17.862839           17.7\n",
      "175330              16.801184           16.5\n",
      "175331              15.508778           15.2\n",
      "175332              13.811304           15.1\n",
      "175333              13.839422           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                11.458157           10.9\n",
      "1                10.070365            9.4\n",
      "2                 9.272301            8.2\n",
      "3                 8.403373            8.1\n",
      "4                 7.927944            7.8\n",
      "...                    ...            ...\n",
      "21911            28.364908           29.0\n",
      "21912            27.740728           27.0\n",
      "21913            26.301588           26.4\n",
      "21914            25.484396           25.6\n",
      "21915            25.103701           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 24.852766           25.4\n",
      "1                 24.571882           25.3\n",
      "2                 24.484844           24.6\n",
      "3                 24.379547           24.4\n",
      "4                 23.907701           24.8\n",
      "...                     ...            ...\n",
      "21913              7.962438            9.8\n",
      "21914              8.697675           10.6\n",
      "21915              9.903104           11.1\n",
      "21916             10.764383           10.5\n",
      "21917             10.063837            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'weighted_mse' saved in '../artifacts/results/cycle_2/test_1/weighted_mse'.\n",
      "\n",
      "Training LSTM model with CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 33ms/step - loss: 111.4523 - mae: 5.7285 - mse: 61.8868 - root_mean_squared_error: 7.4604 - val_loss: 3.2833 - val_mae: 1.1052 - val_mse: 2.0629 - val_root_mean_squared_error: 1.4363 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 31ms/step - loss: 19.0018 - mae: 2.7029 - mse: 12.1557 - root_mean_squared_error: 3.4847 - val_loss: 2.1024 - val_mae: 0.8811 - val_mse: 1.2616 - val_root_mean_squared_error: 1.1232 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 32ms/step - loss: 13.2162 - mae: 2.2474 - mse: 8.4975 - root_mean_squared_error: 2.9144 - val_loss: 2.1296 - val_mae: 0.9005 - val_mse: 1.2843 - val_root_mean_squared_error: 1.1333 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 32ms/step - loss: 9.9135 - mae: 1.9200 - mse: 6.3417 - root_mean_squared_error: 2.5176 - val_loss: 1.4578 - val_mae: 0.6799 - val_mse: 0.8567 - val_root_mean_squared_error: 0.9256 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 33ms/step - loss: 7.6131 - mae: 1.6693 - mse: 4.8388 - root_mean_squared_error: 2.1994 - val_loss: 2.0742 - val_mae: 0.8003 - val_mse: 1.0732 - val_root_mean_squared_error: 1.0360 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 11ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     5.295205            4.9\n",
      "1                     5.330776            4.8\n",
      "2                     5.456803            4.7\n",
      "3                     6.220373            7.1\n",
      "4                    12.633310           13.7\n",
      "...                        ...            ...\n",
      "175329               17.961750           17.7\n",
      "175330               17.324791           16.5\n",
      "175331               15.998284           15.2\n",
      "175332               14.650845           15.1\n",
      "175333               14.215970           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 11.275244           10.9\n",
      "1                  9.849613            9.4\n",
      "2                  8.380085            8.2\n",
      "3                  7.012249            8.1\n",
      "4                  6.817378            7.8\n",
      "...                     ...            ...\n",
      "21911             28.624790           29.0\n",
      "21912             27.469780           27.0\n",
      "21913             26.102028           26.4\n",
      "21914             25.603241           25.6\n",
      "21915             25.200237           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  25.312511           25.4\n",
      "1                  25.121643           25.3\n",
      "2                  25.034508           24.6\n",
      "3                  24.349121           24.4\n",
      "4                  24.357269           24.8\n",
      "...                      ...            ...\n",
      "21913               7.362761            9.8\n",
      "21914               8.288055           10.6\n",
      "21915               8.777540           11.1\n",
      "21916               9.396495           10.5\n",
      "21917               8.827868            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 42ms/step - loss: 71.6463 - mae: 4.5831 - mse: 40.2637 - root_mean_squared_error: 5.9968 - val_loss: 2.6484 - val_mae: 0.9581 - val_mse: 1.4850 - val_root_mean_squared_error: 1.2186 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 33ms/step - loss: 14.9119 - mae: 2.3963 - mse: 9.6218 - root_mean_squared_error: 3.1004 - val_loss: 3.5572 - val_mae: 1.1583 - val_mse: 1.9463 - val_root_mean_squared_error: 1.3951 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 35ms/step - loss: 10.5764 - mae: 2.0168 - mse: 6.8944 - root_mean_squared_error: 2.6250 - val_loss: 1.4208 - val_mae: 0.7146 - val_mse: 0.9231 - val_root_mean_squared_error: 0.9608 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 34ms/step - loss: 8.2822 - mae: 1.7801 - mse: 5.4033 - root_mean_squared_error: 2.3244 - val_loss: 1.4664 - val_mae: 0.7338 - val_mse: 0.9265 - val_root_mean_squared_error: 0.9626 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 38ms/step - loss: 7.3423 - mae: 1.6779 - mse: 4.8218 - root_mean_squared_error: 2.1957 - val_loss: 2.0513 - val_mae: 0.8568 - val_mse: 1.1784 - val_root_mean_squared_error: 1.0856 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 9ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    5.208805            4.9\n",
      "1                    5.225214            4.8\n",
      "2                    5.315536            4.7\n",
      "3                    6.703223            7.1\n",
      "4                   13.514685           13.7\n",
      "...                       ...            ...\n",
      "175329              17.898380           17.7\n",
      "175330              17.303234           16.5\n",
      "175331              15.874197           15.2\n",
      "175332              14.508343           15.1\n",
      "175333              14.166906           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.463276           10.9\n",
      "1                10.283121            9.4\n",
      "2                 8.856790            8.2\n",
      "3                 7.583267            8.1\n",
      "4                 7.318148            7.8\n",
      "...                    ...            ...\n",
      "21911            28.341932           29.0\n",
      "21912            27.547256           27.0\n",
      "21913            25.797071           26.4\n",
      "21914            25.462334           25.6\n",
      "21915            25.039438           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 24.811434           25.4\n",
      "1                 24.526476           25.3\n",
      "2                 24.705555           24.6\n",
      "3                 24.121273           24.4\n",
      "4                 23.906527           24.8\n",
      "...                     ...            ...\n",
      "21913              8.574123            9.8\n",
      "21914              9.133384           10.6\n",
      "21915              9.498083           11.1\n",
      "21916              9.548038           10.5\n",
      "21917              8.964494            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 12ms/step - loss: 48.8360 - mae: 3.8888 - mse: 28.5764 - root_mean_squared_error: 5.1169 - val_loss: 3.2848 - val_mae: 1.0933 - val_mse: 2.0216 - val_root_mean_squared_error: 1.4218 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 10ms/step - loss: 16.9711 - mae: 2.5529 - mse: 10.9945 - root_mean_squared_error: 3.3150 - val_loss: 5.7878 - val_mae: 1.4733 - val_mse: 2.9687 - val_root_mean_squared_error: 1.7230 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 10ms/step - loss: 13.4224 - mae: 2.2516 - mse: 8.6762 - root_mean_squared_error: 2.9451 - val_loss: 3.6153 - val_mae: 1.1554 - val_mse: 1.9361 - val_root_mean_squared_error: 1.3915 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 10ms/step - loss: 11.0280 - mae: 2.0343 - mse: 7.1504 - root_mean_squared_error: 2.6739 - val_loss: 1.7771 - val_mae: 0.7825 - val_mse: 1.0255 - val_root_mean_squared_error: 1.0127 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 10ms/step - loss: 9.6441 - mae: 1.9075 - mse: 6.2904 - root_mean_squared_error: 2.5080 - val_loss: 2.8559 - val_mae: 1.0189 - val_mse: 1.5892 - val_root_mean_squared_error: 1.2606 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    6.997310            4.9\n",
      "1                    6.997310            4.8\n",
      "2                    6.997310            4.7\n",
      "3                    7.100137            7.1\n",
      "4                   13.017122           13.7\n",
      "...                       ...            ...\n",
      "175329              17.621872           17.7\n",
      "175330              16.583954           16.5\n",
      "175331              15.448668           15.2\n",
      "175332              13.439591           15.1\n",
      "175333              13.163746           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                10.998329           10.9\n",
      "1                10.280586            9.4\n",
      "2                 8.664156            8.2\n",
      "3                 7.179667            8.1\n",
      "4                 7.051795            7.8\n",
      "...                    ...            ...\n",
      "21911            27.787163           29.0\n",
      "21912            26.981674           27.0\n",
      "21913            25.886608           26.4\n",
      "21914            24.914940           25.6\n",
      "21915            24.914341           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 24.429459           25.4\n",
      "1                 23.867970           25.3\n",
      "2                 23.896046           24.6\n",
      "3                 23.853870           24.4\n",
      "4                 23.303490           24.8\n",
      "...                     ...            ...\n",
      "21913              7.580720            9.8\n",
      "21914              8.500084           10.6\n",
      "21915              9.714874           11.1\n",
      "21916             10.506760           10.5\n",
      "21917             10.248549            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'custom' saved in '../artifacts/results/cycle_2/test_1/custom'.\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "LSTM:\n",
      "mse: 0.8737\n",
      "mae: 0.6824\n",
      "false_negative_rate: 0.0000\n",
      "\n",
      "GRU:\n",
      "mse: 0.9475\n",
      "mae: 0.7182\n",
      "false_negative_rate: 0.0000\n",
      "\n",
      "CNN:\n",
      "mse: 1.0242\n",
      "mae: 0.7793\n",
      "false_negative_rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv('../artifacts/dataset/01-hourly_historical_analyzed_data.csv')\n",
    "        df = df.drop(columns=['hour', 'day', 'month', 'year'])\n",
    "        \n",
    "        # # testing 0.1\n",
    "        # print(f\"df Dataframe: {df.head()}\")\n",
    "        \n",
    "        # Convert to datetime index\n",
    "        df1 = df.copy()\n",
    "        df1.index = pd.to_datetime(df1['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # # testing 0.2\n",
    "        # print(f\"df1 Dataframe: {df1.head()}\")\n",
    "        \n",
    "        print(\"********DATA INGESTION COMPLETE********\")\n",
    "        \n",
    "        # Extract rain data\n",
    "        rain = df1['rain']\n",
    "        rain_df = pd.DataFrame(rain)\n",
    "        \n",
    "        # Preprocess data with enhanced features\n",
    "        rain_df = preprocess_data(rain_df)\n",
    "        rain_df = rain_df.drop(['rain'], axis=1)\n",
    "        \n",
    "        # # testing 0.3\n",
    "        # print(f\"rain_df Dataframe: {rain_df.head()}\")\n",
    "        \n",
    "        processed_df = pd.concat([df1, rain_df], axis=1)\n",
    "        processed_df = processed_df.drop(['time'], axis=1)\n",
    "        \n",
    "        # # testing 0.4\n",
    "        # print(f\"processed_df Dataframe: {processed_df.head()}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = df_to_X_y(processed_df, window_size=24)\n",
    "        \n",
    "        # # testing 0.5\n",
    "        # print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_time_series_data(X, y)\n",
    "        \n",
    "        # # testing 0.6\n",
    "        # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        # print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        # Scale the data\n",
    "        X_train, X_val, X_test, X_scaler = standardize_features(X_train, X_val, X_test)\n",
    "        \n",
    "        # # testing 0.7\n",
    "        # print(\"Standardization complete\")\n",
    "        \n",
    "        print(\"********DATA PREPROCESSING COMPLETE********\")\n",
    "        \n",
    "        # Define loss functions to iterate over\n",
    "        loss_functions = ['weighted_mse', 'custom']\n",
    "        \n",
    "        for loss_function in loss_functions:\n",
    "            # Create a directory for the current loss function\n",
    "            results_dir = f'../artifacts/results/cycle_2/test_1/{loss_function}'\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "            # Train models\n",
    "            models = {\n",
    "                'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for name, model in models.items():\n",
    "                print(f\"\\nTraining {name.upper()} model with {loss_function.upper()} loss...\")\n",
    "                \n",
    "                try:\n",
    "                    history = train_model(\n",
    "                        model, X_train, y_train, X_val, y_val,\n",
    "                        f'../artifacts/models/cycle_2/test_1/model_{name}_{loss_function}.keras', \n",
    "                        epochs=5, loss_function=loss_function, learning_rate=1e-3\n",
    "                    )\n",
    "                    \n",
    "                    print(\"********MODEL TRAINING COMPLETE********\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error training {name.upper()} model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    results[name] = evaluate_model(model, X_test, y_test)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {name.upper()} model: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Save training history to CSV\n",
    "                history_df = pd.DataFrame(history.history)\n",
    "                history_df.to_csv(os.path.join(results_dir, f'{name}_history.csv'), index=False)\n",
    "                \n",
    "                # Plot training history\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(history.history['loss'], label='Training Loss')\n",
    "                plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                plt.title(f'{name.upper()} Model Training History')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.savefig(os.path.join(results_dir, f'{name}_training_history.png'))\n",
    "                # plt.show()\n",
    "                plt.close()  # Close the plot to free memory\n",
    "                \n",
    "                # Save evaluation results to a text file\n",
    "                with open(os.path.join(results_dir, f'{name}_evaluation.txt'), 'w') as f:\n",
    "                    for metric_name, value in results[name].items():\n",
    "                        f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "                \n",
    "                # Plot predictions for Train, Val, and Test datasets and save the plots\n",
    "                for dataset, data, true_values in zip(['Train', 'Val', 'Test'], \n",
    "                                                    [X_train, X_val, X_test], \n",
    "                                                    [y_train, y_val, y_test]):\n",
    "                    plot_predictions(\n",
    "                        model=model, \n",
    "                        X_data=data, \n",
    "                        y_data=true_values, \n",
    "                        label=name + ' ' + dataset, \n",
    "                        start=100, \n",
    "                        end=500\n",
    "                    )\n",
    "                    plt.savefig(os.path.join(results_dir, f'{name}_{dataset.lower()}_predictions.png'))\n",
    "                    # plt.show()\n",
    "                    plt.close()  # Close the plot to free memory\n",
    "                    \n",
    "                print(\"********MODEL EVALUATION COMPLETE********\")\n",
    "            \n",
    "            print(f\"Results for loss function '{loss_function}' saved in '{results_dir}'.\")\n",
    "            \n",
    "        # Return results\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled error in execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overall best performance\n",
    "1. GRU with weighted loss\n",
    "   * mse: 0.8157\n",
    "   * mae: 0.6507\n",
    "   * false_negative_rate: 0.0000\n",
    "\n",
    "## All models have zero false negative rate: \n",
    "This means none of the models failed to predict rainfall events when they actually occurred, which is particularly important for weather forecasting.\n",
    "\n",
    "## Low values: \n",
    "All the error metrics are relatively low, suggesting that all three models performed reasonably well.\n",
    "\n",
    "## Training curves: \n",
    "All models show good convergence in their training histories, with both training and validation loss decreasing and stabilizing, indicating proper training without significant overfitting.\n",
    "\n",
    "## Visual patterns: \n",
    "From the graphs, all models seem to follow the general pattern of the actual rainfall values, but they struggle with extreme values (particularly low rainfall values where the actual data shows near-zero measurements)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
