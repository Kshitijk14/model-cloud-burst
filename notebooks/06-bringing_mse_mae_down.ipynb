{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    try:\n",
    "        # Process temporal features\n",
    "        df['Seconds'] = df.index.map(pd.Timestamp.timestamp)\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        year = (365.2425) * day\n",
    "        \n",
    "        # Add cyclical time features\n",
    "        df['Day sin'] = np.sin(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Day cos'] = np.cos(df['Seconds'] * (2 * np.pi / day))\n",
    "        df['Year sin'] = np.sin(df['Seconds'] * (2 * np.pi / year))\n",
    "        df['Year cos'] = np.cos(df['Seconds'] * (2 * np.pi / year))\n",
    "        \n",
    "        # # Add rolling statistics\n",
    "        # df['rolling_mean_6h'] = df.iloc[:, 0].rolling(window=6).mean()\n",
    "        # df['rolling_std_6h'] = df.iloc[:, 0].rolling(window=6).std()\n",
    "        # df['rolling_max_6h'] = df.iloc[:, 0].rolling(window=6).max()\n",
    "        \n",
    "        # # Add lag features\n",
    "        # for i in [1, 3, 6, 12]:\n",
    "        #     df[f'lag_{i}'] = df.iloc[:, 0].shift(i)\n",
    "        \n",
    "        df = df.bfill()\n",
    "        df = df.drop(['Seconds'], axis=1)\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def df_to_X_y(df, window_size=24):\n",
    "    try:\n",
    "        df_as_np = df.to_numpy()\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for i in range(len(df_as_np) - window_size):\n",
    "            row = [r for r in df_as_np[i: i + window_size]]\n",
    "            X.append(row)\n",
    "            \n",
    "            label = df_as_np[i + window_size][0]\n",
    "            y.append(label)\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in df_to_X_y: {e}\")\n",
    "        raise\n",
    "\n",
    "def split_time_series_data(X, y, train_ratio=0.8, val_ratio=0.1):\n",
    "    try:\n",
    "        n = len(X)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "        X_train, y_train = X[:train_end], y[:train_end]\n",
    "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "        X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in split_time_series_data: {e}\")\n",
    "        raise\n",
    "\n",
    "def standardize_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Standardize all features in the dataset while maintaining the 3D structure\n",
    "    (samples, timesteps, features).\n",
    "    \"\"\"\n",
    "    num_samples, num_timesteps, num_features = X_train.shape\n",
    "    \n",
    "    # Reshape to 2D for StandardScaler (combine samples & timesteps)\n",
    "    X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "    \n",
    "    # Fit the scaler on training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "    # Transform validation and test sets\n",
    "    X_val_scaled = scaler.transform(X_val.reshape(-1, num_features))\n",
    "    X_test_scaled = scaler.transform(X_test.reshape(-1, num_features))\n",
    "\n",
    "    # Reshape back to original 3D shape\n",
    "    X_train = X_train_scaled.reshape(num_samples, num_timesteps, num_features)\n",
    "    X_val = X_val_scaled.reshape(X_val.shape[0], X_val.shape[1], num_features)\n",
    "    X_test = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], num_features)\n",
    "\n",
    "    return X_train, X_val, X_test, scaler\n",
    "\n",
    "# Model architectures with regularization and dropout\n",
    "def create_lstm_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            LSTM(\n",
    "                128, \n",
    "                return_sequences=True, \n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4),\n",
    "                recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            LSTM(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_lstm_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_gru_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            GRU(\n",
    "                128, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            GRU(\n",
    "                64, \n",
    "                return_sequences=False,\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_gru_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_cnn_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            Conv1D(\n",
    "                filters=128, \n",
    "                kernel_size=3, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Conv1D(\n",
    "                filters=64, \n",
    "                kernel_size=3, \n",
    "                padding='same', \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            Flatten(),\n",
    "            \n",
    "            Dense(\n",
    "                32, \n",
    "                activation='relu',\n",
    "                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)\n",
    "            ),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_cnn_model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Weighted mean squared error\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    try:\n",
    "        weights = tf.where(y_pred < y_true, 2.0, 1.0) # Penalize underestimation more heavily (false negatives)\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        return tf.reduce_mean(weights * squared_difference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in weighted_mse: {e}\")\n",
    "        raise\n",
    "\n",
    "# Custom loss function to penalize false negatives more heavily\n",
    "def custom_loss(y_true, y_pred):\n",
    "    try:\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        mse = tf.reduce_mean(squared_difference, axis=-1)\n",
    "        \n",
    "        # Penalize underestimation more heavily (false negatives)\n",
    "        penalty = tf.where(y_pred < y_true, 2.0, 1.0)\n",
    "        return mse * penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_loss: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Training function with callbacks\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_path, \n",
    "                batch_size=32, epochs=5, patience=3, loss_function='mse', optimizer_function='adam', learning_rate=1e-3):\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, start_from_epoch=3),\n",
    "            ModelCheckpoint(model_path, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=patience, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Select the optimizer based on the input parameter\n",
    "        if optimizer_function == 'adamw':\n",
    "            optimizer = AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n",
    "        elif optimizer_function == 'adam':\n",
    "            optimizer = Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Select the loss function based on the input parameter\n",
    "        if loss_function == 'weighted_mse':\n",
    "            loss = weighted_mse\n",
    "        elif loss_function == 'custom':\n",
    "            loss = custom_loss\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                'mse', \n",
    "                'mae',\n",
    "                'root_mean_squared_error',\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Evaluation function with focus on false negatives\n",
    "# def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "#     try:\n",
    "#         predictions = model.predict(X_test)\n",
    "        \n",
    "#         # Compute MSE and MAE\n",
    "#         mse = np.mean((y_test - predictions.flatten())**2)\n",
    "#         mae = np.mean(np.abs(y_test - predictions.flatten()))\n",
    "        \n",
    "#         # Convert to binary labels\n",
    "#         binary_actual = y_test > threshold\n",
    "#         binary_pred = predictions.flatten() > threshold\n",
    "        \n",
    "#         # Compute false negatives and FNR\n",
    "#         false_negatives = np.sum((binary_actual == True) & (binary_pred == False))\n",
    "#         total_positives = np.sum(binary_actual)\n",
    "#         false_negative_rate = false_negatives / total_positives if total_positives > 0 else 0.0\n",
    "        \n",
    "#         return {\n",
    "#             'mse': mse,\n",
    "#             'mae': mae,\n",
    "#             'false_negative_rate': false_negative_rate\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in evaluate_model: {e}\")\n",
    "#         raise\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluates a time series forecasting model with focus on false negatives\n",
    "    and additional metrics (CSI, POD, FAR, Precision, Recall, F1-score)\n",
    "    using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained TensorFlow Keras model.\n",
    "        X_test (np.ndarray or tf.Tensor): Test data features.  (Assumed already a tensor or convertible)\n",
    "        y_test (np.ndarray or tf.Tensor): Test data labels (actual values). (Assumed already a tensor or convertible)\n",
    "        threshold (float): Threshold to define a cloudburst event.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of evaluation metrics (all TensorFlow tensors).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure X_test and y_test are TensorFlow tensors\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "        y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "        predictions = model.predict(X_test)  # model.predict returns numpy array\n",
    "\n",
    "        # Convert predictions to a TensorFlow tensor\n",
    "        predictions = tf.convert_to_tensor(predictions.flatten(), dtype=tf.float32)\n",
    "\n",
    "        # Compute MSE and MAE (using TensorFlow)\n",
    "        mse = tf.reduce_mean((y_test - predictions)**2)\n",
    "        mae = tf.reduce_mean(tf.abs(y_test - predictions))\n",
    "\n",
    "        # Compute RMSE (using TensorFlow)\n",
    "        rmse = tf.sqrt(mse)  # RMSE is the square root of MSE\n",
    "\n",
    "        # Convert to binary labels (using TensorFlow)\n",
    "        binary_actual = tf.cast(y_test > threshold, tf.float32)\n",
    "        binary_pred = tf.cast(predictions > threshold, tf.float32)\n",
    "\n",
    "        # Calculate True Positives, False Positives, False Negatives, True Negatives\n",
    "        tp = tf.reduce_sum(binary_actual * binary_pred)\n",
    "        fp = tf.reduce_sum((1 - binary_actual) * binary_pred)\n",
    "        fn = tf.reduce_sum(binary_actual * (1 - binary_pred))\n",
    "        tn = tf.reduce_sum((1 - binary_actual) * (1 - binary_pred))\n",
    "\n",
    "        # Handle division by zero\n",
    "        epsilon = 1e-7  # Small constant to prevent division by zero\n",
    "\n",
    "        # Compute additional metrics (using TensorFlow)\n",
    "        csi = tp / (tp + fn + fp + epsilon)\n",
    "        pod = tp / (tp + fn + epsilon)\n",
    "        far = fp / (tp + fp + epsilon)\n",
    "        precision = tp / (tp + fp + epsilon)\n",
    "        recall = tp / (tp + fn + epsilon)  # This is the same as POD\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "        # Compute false negative rate (using TensorFlow)\n",
    "        total_positives = tf.reduce_sum(tf.cast(y_test > threshold, tf.float32))\n",
    "        false_negatives = tf.cast(fn, tf.float32) #Cast to float32 for division\n",
    "        false_negative_rate = false_negatives / (total_positives + epsilon)\n",
    "\n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'false_negative_rate': false_negative_rate,\n",
    "            'CSI': csi,\n",
    "            'POD': pod,\n",
    "            'FAR': far,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise #Re-raise the exception\n",
    "\n",
    "def plot_predictions(model, X_data, y_data, label, start=50, end=500, ylabel='Rainfall (mm)', title_suffix=''):\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_data).flatten()\n",
    "\n",
    "    # Create a DataFrame to store results\n",
    "    results_df = pd.DataFrame(data={f'{label} Predictions': predictions, 'Actual Values': y_data})\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the predictions and actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[f'{label} Predictions'][start:end], label=f'{label} Predictions', color='blue', linestyle='-')\n",
    "    plt.plot(results_df['Actual Values'][start:end], label='Actual Values', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time Stamps', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(f'{label} Predictions vs Actual Values {title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********DATA INGESTION COMPLETE********\n",
      "********DATA PREPROCESSING COMPLETE********\n",
      "\n",
      "Training LSTM model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 32ms/step - loss: 72.0343 - mae: 4.8271 - mse: 43.0378 - root_mean_squared_error: 6.2710 - val_loss: 2.0916 - val_mae: 0.9608 - val_mse: 1.5836 - val_root_mean_squared_error: 1.2584 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 33ms/step - loss: 14.8470 - mae: 2.5191 - mse: 10.5864 - root_mean_squared_error: 3.2520 - val_loss: 1.5929 - val_mae: 0.8436 - val_mse: 1.1963 - val_root_mean_squared_error: 1.0937 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 32ms/step - loss: 10.0476 - mae: 2.0553 - mse: 7.1633 - root_mean_squared_error: 2.6758 - val_loss: 1.5224 - val_mae: 0.7976 - val_mse: 1.1005 - val_root_mean_squared_error: 1.0490 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 32ms/step - loss: 7.9118 - mae: 1.8184 - mse: 5.6315 - root_mean_squared_error: 2.3729 - val_loss: 1.4154 - val_mae: 0.6724 - val_mse: 0.8487 - val_root_mean_squared_error: 0.9212 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 33ms/step - loss: 6.6945 - mae: 1.6850 - mse: 4.8282 - root_mean_squared_error: 2.1973 - val_loss: 2.0642 - val_mae: 0.8628 - val_mse: 1.2502 - val_root_mean_squared_error: 1.1181 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 10ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     6.940198            4.9\n",
      "1                     6.940198            4.8\n",
      "2                     6.940198            4.7\n",
      "3                     7.137085            7.1\n",
      "4                    14.145607           13.7\n",
      "...                        ...            ...\n",
      "175329               17.785906           17.7\n",
      "175330               17.205574           16.5\n",
      "175331               15.622898           15.2\n",
      "175332               14.085207           15.1\n",
      "175333               14.119167           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 10.832623           10.9\n",
      "1                 10.690409            9.4\n",
      "2                  8.327668            8.2\n",
      "3                  7.261298            8.1\n",
      "4                  7.060439            7.8\n",
      "...                     ...            ...\n",
      "21911             28.633690           29.0\n",
      "21912             27.614468           27.0\n",
      "21913             25.849451           26.4\n",
      "21914             25.339230           25.6\n",
      "21915             24.769037           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  24.851252           25.4\n",
      "1                  24.467123           25.3\n",
      "2                  24.514807           24.6\n",
      "3                  23.930584           24.4\n",
      "4                  23.843691           24.8\n",
      "...                      ...            ...\n",
      "21913               7.176719            9.8\n",
      "21914               7.813356           10.6\n",
      "21915               8.528283           11.1\n",
      "21916               9.219900           10.5\n",
      "21917               8.813885            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 31ms/step - loss: 123.5730 - mae: 6.1776 - mse: 70.9720 - root_mean_squared_error: 8.0152 - val_loss: 2.0745 - val_mae: 1.0134 - val_mse: 1.6996 - val_root_mean_squared_error: 1.3037 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 31ms/step - loss: 16.0919 - mae: 2.6276 - mse: 11.5320 - root_mean_squared_error: 3.3943 - val_loss: 1.5609 - val_mae: 0.9012 - val_mse: 1.3183 - val_root_mean_squared_error: 1.1482 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - loss: 11.1559 - mae: 2.1747 - mse: 8.0033 - root_mean_squared_error: 2.8282 - val_loss: 1.3926 - val_mae: 0.7712 - val_mse: 1.0687 - val_root_mean_squared_error: 1.0338 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - loss: 8.5377 - mae: 1.8999 - mse: 6.1569 - root_mean_squared_error: 2.4810 - val_loss: 1.5865 - val_mae: 0.8573 - val_mse: 1.2948 - val_root_mean_squared_error: 1.1379 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 31ms/step - loss: 7.0953 - mae: 1.7524 - mse: 5.2156 - root_mean_squared_error: 2.2837 - val_loss: 1.7105 - val_mae: 0.8087 - val_mse: 1.1368 - val_root_mean_squared_error: 1.0662 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m752s\u001b[0m 137ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    5.593570            4.9\n",
      "1                    5.593570            4.8\n",
      "2                    5.593570            4.7\n",
      "3                    5.823463            7.1\n",
      "4                   13.535325           13.7\n",
      "...                       ...            ...\n",
      "175329              17.951670           17.7\n",
      "175330              17.304085           16.5\n",
      "175331              15.910224           15.2\n",
      "175332              14.448058           15.1\n",
      "175333              14.387240           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.543120           10.9\n",
      "1                10.807051            9.4\n",
      "2                 9.043588            8.2\n",
      "3                 7.767229            8.1\n",
      "4                 7.190619            7.8\n",
      "...                    ...            ...\n",
      "21911            28.381031           29.0\n",
      "21912            27.732185           27.0\n",
      "21913            26.446381           26.4\n",
      "21914            25.836403           25.6\n",
      "21915            25.349972           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.263577           25.4\n",
      "1                 24.764744           25.3\n",
      "2                 24.811890           24.6\n",
      "3                 24.386295           24.4\n",
      "4                 24.225853           24.8\n",
      "...                     ...            ...\n",
      "21913              7.074612            9.8\n",
      "21914              7.967487           10.6\n",
      "21915              8.873169           11.1\n",
      "21916              9.595312           10.5\n",
      "21917              8.994358            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with ADAM optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 8ms/step - loss: 51.1956 - mae: 4.1507 - mse: 32.1858 - root_mean_squared_error: 5.4647 - val_loss: 2.3643 - val_mae: 0.9414 - val_mse: 1.4839 - val_root_mean_squared_error: 1.2181 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 18.1835 - mae: 2.7640 - mse: 12.9156 - root_mean_squared_error: 3.5932 - val_loss: 2.0548 - val_mae: 1.0089 - val_mse: 1.6451 - val_root_mean_squared_error: 1.2826 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 14.4590 - mae: 2.4473 - mse: 10.2453 - root_mean_squared_error: 3.2003 - val_loss: 1.6532 - val_mae: 0.7356 - val_mse: 0.9489 - val_root_mean_squared_error: 0.9741 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 11.3853 - mae: 2.1509 - mse: 8.0444 - root_mean_squared_error: 2.8359 - val_loss: 1.5260 - val_mae: 0.7316 - val_mse: 0.9464 - val_root_mean_squared_error: 0.9728 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - loss: 9.0835 - mae: 1.9104 - mse: 6.4226 - root_mean_squared_error: 2.5340 - val_loss: 1.3018 - val_mae: 0.7028 - val_mse: 0.9163 - val_root_mean_squared_error: 0.9572 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    5.218269            4.9\n",
      "1                    5.419634            4.8\n",
      "2                    5.849678            4.7\n",
      "3                    8.148422            7.1\n",
      "4                   13.436121           13.7\n",
      "...                       ...            ...\n",
      "175329              17.637482           17.7\n",
      "175330              16.906485           16.5\n",
      "175331              15.432686           15.2\n",
      "175332              13.816410           15.1\n",
      "175333              13.546845           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                11.091519           10.9\n",
      "1                10.212261            9.4\n",
      "2                 9.215301            8.2\n",
      "3                 8.240805            8.1\n",
      "4                 7.864072            7.8\n",
      "...                    ...            ...\n",
      "21911            28.642101           29.0\n",
      "21912            27.824444           27.0\n",
      "21913            26.452919           26.4\n",
      "21914            25.597471           25.6\n",
      "21915            25.153431           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 24.874733           25.4\n",
      "1                 24.732412           25.3\n",
      "2                 24.783424           24.6\n",
      "3                 24.091085           24.4\n",
      "4                 23.965834           24.8\n",
      "...                     ...            ...\n",
      "21913              7.512189            9.8\n",
      "21914              8.747561           10.6\n",
      "21915             10.597244           11.1\n",
      "21916             10.899100           10.5\n",
      "21917             10.368232            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training LSTM model with ADAM optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 33ms/step - loss: 78.5485 - mae: 4.9060 - mse: 44.6115 - root_mean_squared_error: 6.3791 - val_loss: 3.2910 - val_mae: 1.0900 - val_mse: 2.0812 - val_root_mean_squared_error: 1.4427 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 32ms/step - loss: 17.2222 - mae: 2.5705 - mse: 11.0553 - root_mean_squared_error: 3.3224 - val_loss: 3.0707 - val_mae: 0.9774 - val_mse: 1.7567 - val_root_mean_squared_error: 1.3254 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 33ms/step - loss: 11.8989 - mae: 2.1266 - mse: 7.6590 - root_mean_squared_error: 2.7668 - val_loss: 1.9243 - val_mae: 0.7892 - val_mse: 1.1090 - val_root_mean_squared_error: 1.0531 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 34ms/step - loss: 9.1149 - mae: 1.8403 - mse: 5.8266 - root_mean_squared_error: 2.4135 - val_loss: 1.7426 - val_mae: 0.7294 - val_mse: 1.0004 - val_root_mean_squared_error: 1.0002 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 34ms/step - loss: 7.5775 - mae: 1.6700 - mse: 4.8089 - root_mean_squared_error: 2.1924 - val_loss: 1.5707 - val_mae: 0.7071 - val_mse: 0.8438 - val_root_mean_squared_error: 0.9186 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 10ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     5.896343            4.9\n",
      "1                     5.913676            4.8\n",
      "2                     5.972197            4.7\n",
      "3                     6.887171            7.1\n",
      "4                    13.557281           13.7\n",
      "...                        ...            ...\n",
      "175329               17.976460           17.7\n",
      "175330               17.313538           16.5\n",
      "175331               15.422865           15.2\n",
      "175332               13.834564           15.1\n",
      "175333               14.224440           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 10.394927           10.9\n",
      "1                 10.811676            9.4\n",
      "2                  8.635390            8.2\n",
      "3                  8.191728            8.1\n",
      "4                  7.532729            7.8\n",
      "...                     ...            ...\n",
      "21911             28.299801           29.0\n",
      "21912             27.164505           27.0\n",
      "21913             25.537910           26.4\n",
      "21914             24.967091           25.6\n",
      "21915             24.265945           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  24.469303           25.4\n",
      "1                  24.222603           25.3\n",
      "2                  24.157310           24.6\n",
      "3                  23.368782           24.4\n",
      "4                  23.482613           24.8\n",
      "...                      ...            ...\n",
      "21913               7.577585            9.8\n",
      "21914               8.753387           10.6\n",
      "21915               9.935506           11.1\n",
      "21916              10.459812           10.5\n",
      "21917              10.045444            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAM optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 39ms/step - loss: 83.4074 - mae: 4.9468 - mse: 46.7978 - root_mean_squared_error: 6.4748 - val_loss: 3.1921 - val_mae: 1.0480 - val_mse: 2.0319 - val_root_mean_squared_error: 1.4254 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 33ms/step - loss: 15.5877 - mae: 2.4585 - mse: 10.0539 - root_mean_squared_error: 3.1694 - val_loss: 3.3631 - val_mae: 1.0967 - val_mse: 1.8146 - val_root_mean_squared_error: 1.3471 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m841s\u001b[0m 153ms/step - loss: 10.9754 - mae: 2.0453 - mse: 7.1234 - root_mean_squared_error: 2.6684 - val_loss: 1.9203 - val_mae: 0.7921 - val_mse: 1.0714 - val_root_mean_squared_error: 1.0351 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 34ms/step - loss: 8.8360 - mae: 1.8362 - mse: 5.7645 - root_mean_squared_error: 2.4003 - val_loss: 1.5481 - val_mae: 0.7336 - val_mse: 0.9423 - val_root_mean_squared_error: 0.9707 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 35ms/step - loss: 7.4871 - mae: 1.6909 - mse: 4.9029 - root_mean_squared_error: 2.2142 - val_loss: 2.5543 - val_mae: 0.9605 - val_mse: 1.4092 - val_root_mean_squared_error: 1.1871 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    6.572315            4.9\n",
      "1                    6.572315            4.8\n",
      "2                    6.572315            4.7\n",
      "3                    6.572315            7.1\n",
      "4                   13.274055           13.7\n",
      "...                       ...            ...\n",
      "175329              17.691250           17.7\n",
      "175330              16.619371           16.5\n",
      "175331              14.872311           15.2\n",
      "175332              13.777328           15.1\n",
      "175333              14.174935           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                10.766428           10.9\n",
      "1                10.100611            9.4\n",
      "2                 8.747750            8.2\n",
      "3                 6.736519            8.1\n",
      "4                 6.572315            7.8\n",
      "...                    ...            ...\n",
      "21911            28.411362           29.0\n",
      "21912            27.497200           27.0\n",
      "21913            26.135338           26.4\n",
      "21914            25.710602           25.6\n",
      "21915            25.547539           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.612568           25.4\n",
      "1                 25.241316           25.3\n",
      "2                 25.202776           24.6\n",
      "3                 24.711765           24.4\n",
      "4                 24.421200           24.8\n",
      "...                     ...            ...\n",
      "21913              6.572315            9.8\n",
      "21914              6.623376           10.6\n",
      "21915              7.133698           11.1\n",
      "21916              7.984746           10.5\n",
      "21917              7.862300            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with ADAM optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 8ms/step - loss: 61.9152 - mae: 4.2486 - mse: 35.5255 - root_mean_squared_error: 5.6416 - val_loss: 5.3921 - val_mae: 1.3532 - val_mse: 2.8492 - val_root_mean_squared_error: 1.6880 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step - loss: 20.2489 - mae: 2.7639 - mse: 13.0465 - root_mean_squared_error: 3.6110 - val_loss: 2.3125 - val_mae: 0.8945 - val_mse: 1.3243 - val_root_mean_squared_error: 1.1508 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step - loss: 15.6970 - mae: 2.4102 - mse: 10.1095 - root_mean_squared_error: 3.1791 - val_loss: 1.6345 - val_mae: 0.7644 - val_mse: 1.0188 - val_root_mean_squared_error: 1.0094 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step - loss: 12.5750 - mae: 2.1396 - mse: 8.0961 - root_mean_squared_error: 2.8447 - val_loss: 1.8183 - val_mae: 0.7998 - val_mse: 1.0492 - val_root_mean_squared_error: 1.0243 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 8ms/step - loss: 10.0083 - mae: 1.8997 - mse: 6.4379 - root_mean_squared_error: 2.5368 - val_loss: 2.2164 - val_mae: 0.8852 - val_mse: 1.1974 - val_root_mean_squared_error: 1.0942 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    4.439428            4.9\n",
      "1                    4.221607            4.8\n",
      "2                    4.551733            4.7\n",
      "3                    6.993581            7.1\n",
      "4                   13.073067           13.7\n",
      "...                       ...            ...\n",
      "175329              17.777229           17.7\n",
      "175330              17.153103           16.5\n",
      "175331              15.894431           15.2\n",
      "175332              14.148102           15.1\n",
      "175333              14.286973           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                11.982653           10.9\n",
      "1                10.560146            9.4\n",
      "2                 9.478676            8.2\n",
      "3                 7.858556            8.1\n",
      "4                 7.499601            7.8\n",
      "...                    ...            ...\n",
      "21911            28.038990           29.0\n",
      "21912            27.200630           27.0\n",
      "21913            26.265297           26.4\n",
      "21914            25.273487           25.6\n",
      "21915            25.201820           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 24.819603           25.4\n",
      "1                 24.611038           25.3\n",
      "2                 24.122131           24.6\n",
      "3                 24.091606           24.4\n",
      "4                 23.604931           24.8\n",
      "...                     ...            ...\n",
      "21913              8.218616            9.8\n",
      "21914              9.293613           10.6\n",
      "21915             10.257854           11.1\n",
      "21916             10.598242           10.5\n",
      "21917             10.178829            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'custom' saved in '../artifacts/results/cycle_2/test_3/adam_custom'.\n",
      "\n",
      "Training LSTM model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 51ms/step - loss: 89.0739 - mae: 5.1882 - mse: 51.8681 - root_mean_squared_error: 6.8217 - val_loss: 1.6042 - val_mae: 0.7413 - val_mse: 0.9832 - val_root_mean_squared_error: 0.9916 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 71ms/step - loss: 14.6377 - mae: 2.4964 - mse: 10.4284 - root_mean_squared_error: 3.2285 - val_loss: 2.3951 - val_mae: 1.1411 - val_mse: 2.1074 - val_root_mean_squared_error: 1.4517 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 107ms/step - loss: 10.5611 - mae: 2.0926 - mse: 7.4885 - root_mean_squared_error: 2.7354 - val_loss: 1.8036 - val_mae: 0.9855 - val_mse: 1.5009 - val_root_mean_squared_error: 1.2251 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 107ms/step - loss: 7.8047 - mae: 1.7912 - mse: 5.5339 - root_mean_squared_error: 2.3521 - val_loss: 1.7315 - val_mae: 0.8418 - val_mse: 1.1908 - val_root_mean_squared_error: 1.0912 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 112ms/step - loss: 6.5833 - mae: 1.6432 - mse: 4.6827 - root_mean_squared_error: 2.1637 - val_loss: 1.1794 - val_mae: 0.6105 - val_mse: 0.7535 - val_root_mean_squared_error: 0.8680 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 23ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     5.991708            4.9\n",
      "1                     6.026029            4.8\n",
      "2                     6.252727            4.7\n",
      "3                     8.213627            7.1\n",
      "4                    13.997255           13.7\n",
      "...                        ...            ...\n",
      "175329               17.771957           17.7\n",
      "175330               17.133051           16.5\n",
      "175331               15.289190           15.2\n",
      "175332               14.010794           15.1\n",
      "175333               13.745546           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 10.173827           10.9\n",
      "1                  9.748103            9.4\n",
      "2                  8.629786            8.2\n",
      "3                  7.980506            8.1\n",
      "4                  7.725562            7.8\n",
      "...                     ...            ...\n",
      "21911             28.882444           29.0\n",
      "21912             28.035904           27.0\n",
      "21913             26.410641           26.4\n",
      "21914             25.846064           25.6\n",
      "21915             25.421793           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  25.391306           25.4\n",
      "1                  24.836596           25.3\n",
      "2                  24.931643           24.6\n",
      "3                  24.459652           24.4\n",
      "4                  24.190115           24.8\n",
      "...                      ...            ...\n",
      "21913               8.007469            9.8\n",
      "21914               8.445985           10.6\n",
      "21915               8.960991           11.1\n",
      "21916               9.438290           10.5\n",
      "21917               9.037801            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 63ms/step - loss: 94.4730 - mae: 5.3907 - mse: 54.8194 - root_mean_squared_error: 7.0210 - val_loss: 2.0667 - val_mae: 0.9980 - val_mse: 1.6194 - val_root_mean_squared_error: 1.2726 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 57ms/step - loss: 15.7852 - mae: 2.5985 - mse: 11.2864 - root_mean_squared_error: 3.3581 - val_loss: 1.6486 - val_mae: 0.8154 - val_mse: 1.1393 - val_root_mean_squared_error: 1.0674 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 58ms/step - loss: 10.9375 - mae: 2.1491 - mse: 7.8571 - root_mean_squared_error: 2.8025 - val_loss: 1.3659 - val_mae: 0.7138 - val_mse: 0.9564 - val_root_mean_squared_error: 0.9779 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 61ms/step - loss: 8.3005 - mae: 1.8760 - mse: 6.0083 - root_mean_squared_error: 2.4510 - val_loss: 1.5860 - val_mae: 0.8183 - val_mse: 1.2074 - val_root_mean_squared_error: 1.0988 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 59ms/step - loss: 7.1102 - mae: 1.7546 - mse: 5.2272 - root_mean_squared_error: 2.2862 - val_loss: 1.7629 - val_mae: 0.8826 - val_mse: 1.3611 - val_root_mean_squared_error: 1.1667 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    5.586830            4.9\n",
      "1                    5.586830            4.8\n",
      "2                    5.586830            4.7\n",
      "3                    5.586830            7.1\n",
      "4                   12.840503           13.7\n",
      "...                       ...            ...\n",
      "175329              17.385183           17.7\n",
      "175330              16.759899           16.5\n",
      "175331              15.393981           15.2\n",
      "175332              13.475948           15.1\n",
      "175333              14.158819           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                10.849867           10.9\n",
      "1                 9.057673            9.4\n",
      "2                 7.885264            8.2\n",
      "3                 6.964155            8.1\n",
      "4                 6.681939            7.8\n",
      "...                    ...            ...\n",
      "21911            29.611824           29.0\n",
      "21912            28.671053           27.0\n",
      "21913            26.818844           26.4\n",
      "21914            26.186310           25.6\n",
      "21915            25.679005           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.592957           25.4\n",
      "1                 24.892044           25.3\n",
      "2                 24.811485           24.6\n",
      "3                 24.313217           24.4\n",
      "4                 24.135654           24.8\n",
      "...                     ...            ...\n",
      "21913              7.385352            9.8\n",
      "21914              8.213059           10.6\n",
      "21915              8.874144           11.1\n",
      "21916              9.330084           10.5\n",
      "21917              8.956184            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with ADAMW optimizer and WEIGHTED_MSE loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 15ms/step - loss: 61.9547 - mae: 4.4926 - mse: 38.0433 - root_mean_squared_error: 5.9141 - val_loss: 5.2700 - val_mae: 1.3655 - val_mse: 2.8430 - val_root_mean_squared_error: 1.6861 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 15ms/step - loss: 22.4989 - mae: 3.0351 - mse: 15.7223 - root_mean_squared_error: 3.9640 - val_loss: 1.8954 - val_mae: 0.8403 - val_mse: 1.2343 - val_root_mean_squared_error: 1.1110 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 15ms/step - loss: 17.1899 - mae: 2.6294 - mse: 11.9825 - root_mean_squared_error: 3.4607 - val_loss: 1.7096 - val_mae: 0.7990 - val_mse: 1.0951 - val_root_mean_squared_error: 1.0464 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 15ms/step - loss: 13.1928 - mae: 2.2872 - mse: 9.1990 - root_mean_squared_error: 3.0321 - val_loss: 1.5306 - val_mae: 0.7490 - val_mse: 0.9896 - val_root_mean_squared_error: 0.9948 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 15ms/step - loss: 9.9348 - mae: 1.9823 - mse: 6.9689 - root_mean_squared_error: 2.6396 - val_loss: 1.3348 - val_mae: 0.7175 - val_mse: 0.9032 - val_root_mean_squared_error: 0.9504 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 5ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    5.139567            4.9\n",
      "1                    5.105472            4.8\n",
      "2                    5.310670            4.7\n",
      "3                    7.034453            7.1\n",
      "4                   12.772623           13.7\n",
      "...                       ...            ...\n",
      "175329              17.779758           17.7\n",
      "175330              17.039707           16.5\n",
      "175331              15.899761           15.2\n",
      "175332              13.977306           15.1\n",
      "175333              13.947309           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                11.540839           10.9\n",
      "1                10.759942            9.4\n",
      "2                 9.502388            8.2\n",
      "3                 8.791193            8.1\n",
      "4                 8.368448            7.8\n",
      "...                    ...            ...\n",
      "21911            28.253275           29.0\n",
      "21912            27.566122           27.0\n",
      "21913            26.563496           26.4\n",
      "21914            25.721632           25.6\n",
      "21915            25.202055           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 25.023691           25.4\n",
      "1                 24.721004           25.3\n",
      "2                 24.567471           24.6\n",
      "3                 24.291428           24.4\n",
      "4                 23.741287           24.8\n",
      "...                     ...            ...\n",
      "21913              8.281932            9.8\n",
      "21914              9.240976           10.6\n",
      "21915             10.067574           11.1\n",
      "21916             10.431810           10.5\n",
      "21917              9.875630            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training LSTM model with ADAMW optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 90ms/step - loss: 99.6384 - mae: 5.4467 - mse: 55.9182 - root_mean_squared_error: 7.1150 - val_loss: 2.4669 - val_mae: 1.1189 - val_mse: 1.9640 - val_root_mean_squared_error: 1.4014 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 88ms/step - loss: 17.9359 - mae: 2.6435 - mse: 11.5549 - root_mean_squared_error: 3.3977 - val_loss: 1.8805 - val_mae: 0.7972 - val_mse: 1.1723 - val_root_mean_squared_error: 1.0827 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 90ms/step - loss: 13.3229 - mae: 2.2709 - mse: 8.6329 - root_mean_squared_error: 2.9378 - val_loss: 2.0914 - val_mae: 0.8216 - val_mse: 1.1542 - val_root_mean_squared_error: 1.0743 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 90ms/step - loss: 10.5195 - mae: 1.9934 - mse: 6.7756 - root_mean_squared_error: 2.6027 - val_loss: 2.0883 - val_mae: 0.8897 - val_mse: 1.2557 - val_root_mean_squared_error: 1.1206 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 92ms/step - loss: 8.2282 - mae: 1.7696 - mse: 5.3026 - root_mean_squared_error: 2.3025 - val_loss: 1.9696 - val_mae: 0.7992 - val_mse: 1.1388 - val_root_mean_squared_error: 1.0671 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 23ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 22ms/step\n",
      "        lstm Train Predictions  Actual Values\n",
      "0                     4.254589            4.9\n",
      "1                     4.442581            4.8\n",
      "2                     4.702746            4.7\n",
      "3                     5.329246            7.1\n",
      "4                    13.321373           13.7\n",
      "...                        ...            ...\n",
      "175329               17.653336           17.7\n",
      "175330               17.082985           16.5\n",
      "175331               15.568075           15.2\n",
      "175332               13.850204           15.1\n",
      "175333               13.636282           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 23ms/step\n",
      "       lstm Val Predictions  Actual Values\n",
      "0                 11.013665           10.9\n",
      "1                 10.331454            9.4\n",
      "2                  8.742426            8.2\n",
      "3                  7.385192            8.1\n",
      "4                  6.314943            7.8\n",
      "...                     ...            ...\n",
      "21911             29.013369           29.0\n",
      "21912             28.164558           27.0\n",
      "21913             26.705547           26.4\n",
      "21914             25.913406           25.6\n",
      "21915             25.143560           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 23ms/step\n",
      "       lstm Test Predictions  Actual Values\n",
      "0                  24.762211           25.4\n",
      "1                  24.559500           25.3\n",
      "2                  24.444103           24.6\n",
      "3                  23.939592           24.4\n",
      "4                  24.062592           24.8\n",
      "...                      ...            ...\n",
      "21913               6.140382            9.8\n",
      "21914               6.636400           10.6\n",
      "21915               7.192046           11.1\n",
      "21916               7.702847           10.5\n",
      "21917               7.765376            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training GRU model with ADAMW optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 101ms/step - loss: 72.8387 - mae: 4.6896 - mse: 41.4477 - root_mean_squared_error: 6.1267 - val_loss: 2.1294 - val_mae: 0.8787 - val_mse: 1.3696 - val_root_mean_squared_error: 1.1703 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 85ms/step - loss: 14.8856 - mae: 2.4003 - mse: 9.6273 - root_mean_squared_error: 3.1019 - val_loss: 1.8473 - val_mae: 0.8615 - val_mse: 1.2653 - val_root_mean_squared_error: 1.1249 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m566s\u001b[0m 103ms/step - loss: 10.8584 - mae: 2.0342 - mse: 7.0498 - root_mean_squared_error: 2.6545 - val_loss: 1.2936 - val_mae: 0.7325 - val_mse: 0.9482 - val_root_mean_squared_error: 0.9738 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 110ms/step - loss: 8.5031 - mae: 1.7896 - mse: 5.5297 - root_mean_squared_error: 2.3512 - val_loss: 1.7692 - val_mae: 0.7983 - val_mse: 1.0676 - val_root_mean_squared_error: 1.0333 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 109ms/step - loss: 7.1435 - mae: 1.6363 - mse: 4.6449 - root_mean_squared_error: 2.1551 - val_loss: 1.5477 - val_mae: 0.7179 - val_mse: 0.9077 - val_root_mean_squared_error: 0.9527 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 21ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 21ms/step\n",
      "        gru Train Predictions  Actual Values\n",
      "0                    4.401346            4.9\n",
      "1                    4.439098            4.8\n",
      "2                    4.510282            4.7\n",
      "3                    6.175478            7.1\n",
      "4                   13.634836           13.7\n",
      "...                       ...            ...\n",
      "175329              18.366467           17.7\n",
      "175330              17.653877           16.5\n",
      "175331              15.733101           15.2\n",
      "175332              13.791458           15.1\n",
      "175333              14.688215           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 21ms/step\n",
      "       gru Val Predictions  Actual Values\n",
      "0                11.751965           10.9\n",
      "1                 9.907052            9.4\n",
      "2                 8.903646            8.2\n",
      "3                 7.655661            8.1\n",
      "4                 7.045274            7.8\n",
      "...                    ...            ...\n",
      "21911            28.960676           29.0\n",
      "21912            28.211843           27.0\n",
      "21913            26.821825           26.4\n",
      "21914            26.169809           25.6\n",
      "21915            25.782724           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 20ms/step\n",
      "       gru Test Predictions  Actual Values\n",
      "0                 25.787348           25.4\n",
      "1                 25.285307           25.3\n",
      "2                 25.219830           24.6\n",
      "3                 24.746840           24.4\n",
      "4                 24.524687           24.8\n",
      "...                     ...            ...\n",
      "21913              6.959414            9.8\n",
      "21914              7.987393           10.6\n",
      "21915              8.934103           11.1\n",
      "21916              9.516531           10.5\n",
      "21917              9.241803            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "\n",
      "Training CNN model with ADAMW optimizer and CUSTOM loss...\n",
      "Epoch 1/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 30ms/step - loss: 63.8149 - mae: 4.3280 - mse: 36.5992 - root_mean_squared_error: 5.7444 - val_loss: 5.8691 - val_mae: 1.4446 - val_mse: 3.0567 - val_root_mean_squared_error: 1.7483 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 27ms/step - loss: 21.6453 - mae: 2.8492 - mse: 13.9174 - root_mean_squared_error: 3.7299 - val_loss: 3.8505 - val_mae: 1.1908 - val_mse: 2.1061 - val_root_mean_squared_error: 1.4512 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 24ms/step - loss: 16.4748 - mae: 2.4620 - mse: 10.6260 - root_mean_squared_error: 3.2587 - val_loss: 1.9810 - val_mae: 0.8231 - val_mse: 1.1217 - val_root_mean_squared_error: 1.0591 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 24ms/step - loss: 12.7353 - mae: 2.1549 - mse: 8.2042 - root_mean_squared_error: 2.8638 - val_loss: 2.4153 - val_mae: 0.9202 - val_mse: 1.3516 - val_root_mean_squared_error: 1.1626 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 24ms/step - loss: 9.9319 - mae: 1.8903 - mse: 6.3952 - root_mean_squared_error: 2.5284 - val_loss: 3.4755 - val_mae: 1.1161 - val_mse: 1.8229 - val_root_mean_squared_error: 1.3501 - learning_rate: 0.0010\n",
      "********MODEL TRAINING COMPLETE********\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "\u001b[1m5480/5480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 7ms/step\n",
      "        cnn Train Predictions  Actual Values\n",
      "0                    4.445218            4.9\n",
      "1                    4.418555            4.8\n",
      "2                    4.759397            4.7\n",
      "3                    7.146707            7.1\n",
      "4                   12.408520           13.7\n",
      "...                       ...            ...\n",
      "175329              17.540825           17.7\n",
      "175330              16.793566           16.5\n",
      "175331              15.812464           15.2\n",
      "175332              14.127411           15.1\n",
      "175333              13.316838           11.1\n",
      "\n",
      "[175334 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n",
      "       cnn Val Predictions  Actual Values\n",
      "0                11.488234           10.9\n",
      "1                 9.967993            9.4\n",
      "2                 8.993245            8.2\n",
      "3                 7.133995            8.1\n",
      "4                 6.876862            7.8\n",
      "...                    ...            ...\n",
      "21911            27.432180           29.0\n",
      "21912            26.950710           27.0\n",
      "21913            25.354061           26.4\n",
      "21914            25.140285           25.6\n",
      "21915            24.663254           25.6\n",
      "\n",
      "[21916 rows x 2 columns]\n",
      "\u001b[1m685/685\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step\n",
      "       cnn Test Predictions  Actual Values\n",
      "0                 24.795593           25.4\n",
      "1                 24.203642           25.3\n",
      "2                 24.245941           24.6\n",
      "3                 23.935575           24.4\n",
      "4                 23.734058           24.8\n",
      "...                     ...            ...\n",
      "21913              7.847299            9.8\n",
      "21914              8.794523           10.6\n",
      "21915              9.739041           11.1\n",
      "21916              9.967329           10.5\n",
      "21917             10.045592            9.1\n",
      "\n",
      "[21918 rows x 2 columns]\n",
      "********MODEL EVALUATION COMPLETE********\n",
      "Results for loss function 'custom' saved in '../artifacts/results/cycle_2/test_3/adamw_custom'.\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "LSTM:\n",
      "mse: 1.2319\n",
      "mae: 0.8128\n",
      "rmse: 1.1099\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "GRU:\n",
      "mse: 0.9689\n",
      "mae: 0.7422\n",
      "rmse: 0.9843\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "\n",
      "CNN:\n",
      "mse: 1.1089\n",
      "mae: 0.8162\n",
      "rmse: 1.0530\n",
      "false_negative_rate: 0.0000\n",
      "CSI: 1.0000\n",
      "POD: 1.0000\n",
      "FAR: 0.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv('../artifacts/dataset/01-hourly_historical_analyzed_data.csv')\n",
    "        df = df.drop(columns=['hour', 'day', 'month', 'year'])\n",
    "        \n",
    "        # # testing 0.1\n",
    "        # print(f\"df Dataframe: {df.head()}\")\n",
    "        \n",
    "        # Convert to datetime index\n",
    "        df1 = df.copy()\n",
    "        df1.index = pd.to_datetime(df1['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # # testing 0.2\n",
    "        # print(f\"df1 Dataframe: {df1.head()}\")\n",
    "        \n",
    "        print(\"********DATA INGESTION COMPLETE********\")\n",
    "        \n",
    "        # Extract rain data\n",
    "        rain = df1['rain']\n",
    "        rain_df = pd.DataFrame(rain)\n",
    "        \n",
    "        # Preprocess data with enhanced features\n",
    "        rain_df = preprocess_data(rain_df)\n",
    "        rain_df = rain_df.drop(['rain'], axis=1)\n",
    "        \n",
    "        # # testing 0.3\n",
    "        # print(f\"rain_df Dataframe: {rain_df.head()}\")\n",
    "        \n",
    "        processed_df = pd.concat([df1, rain_df], axis=1)\n",
    "        processed_df = processed_df.drop(['time'], axis=1)\n",
    "        \n",
    "        # # testing 0.4\n",
    "        # print(f\"processed_df Dataframe: {processed_df.head()}\")\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = df_to_X_y(processed_df, window_size=24)\n",
    "        \n",
    "        # # testing 0.5\n",
    "        # print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = split_time_series_data(X, y)\n",
    "        \n",
    "        # # testing 0.6\n",
    "        # print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        # print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        # print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "        # Scale the data\n",
    "        X_train, X_val, X_test, X_scaler = standardize_features(X_train, X_val, X_test)\n",
    "        \n",
    "        # # testing 0.7\n",
    "        # print(\"Standardization complete\")\n",
    "        \n",
    "        print(\"********DATA PREPROCESSING COMPLETE********\")\n",
    "        \n",
    "        # Define optimizers and loss functions to iterate over\n",
    "        optimizers = ['adam', 'adamw']\n",
    "        loss_functions = ['weighted_mse', 'custom']\n",
    "        \n",
    "        # for loss_function in loss_functions:\n",
    "        \n",
    "        #     # Create a directory for the current loss function\n",
    "        #     results_dir = f'../artifacts/results/cycle_2/test_3/{loss_function}'\n",
    "        #     os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        #     # Train models\n",
    "        #     models = {\n",
    "        #         'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #         'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #         'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        #     }\n",
    "            \n",
    "        #     results = {}\n",
    "        #     for name, model in models.items():\n",
    "        #         print(f\"\\nTraining {name.upper()} model with {loss_function.upper()} loss...\")\n",
    "                \n",
    "        #         try:\n",
    "        #             history = train_model(\n",
    "        #                 model, X_train, y_train, X_val, y_val,\n",
    "        #                 f'../artifacts/models/cycle_2/test_3/model_{name}_{loss_function}.keras', \n",
    "        #                 epochs=5, loss_function=loss_function, learning_rate=1e-3\n",
    "        #             )\n",
    "                    \n",
    "        #             print(\"********MODEL TRAINING COMPLETE********\")\n",
    "        #         except Exception as e:\n",
    "        #             print(f\"Error training {name.upper()} model: {e}\")\n",
    "        #             continue\n",
    "        \n",
    "        for optimizer_function in optimizers:\n",
    "            for loss_function in loss_functions:\n",
    "                \n",
    "                # Create a directory for the current optimizer-loss function combination\n",
    "                results_dir = f'../artifacts/results/cycle_2/test_3/{optimizer_function}_{loss_function}'\n",
    "                os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "                # Train models\n",
    "                models = {\n",
    "                    'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                    'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                }\n",
    "\n",
    "                results = {}\n",
    "                for name, model in models.items():\n",
    "                    print(f\"\\nTraining {name.upper()} model with {optimizer_function.upper()} optimizer and {loss_function.upper()} loss...\")\n",
    "\n",
    "                    try:\n",
    "                        history = train_model(\n",
    "                            model, X_train, y_train, X_val, y_val,\n",
    "                            f'../artifacts/models/cycle_2/test_3/model_{name}_{optimizer_function}_{loss_function}.keras', \n",
    "                            epochs=5, loss_function=loss_function, optimizer_function=optimizer_function, learning_rate=1e-3\n",
    "                        )\n",
    "\n",
    "                        print(\"********MODEL TRAINING COMPLETE********\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error training {name.upper()} model: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                    try:\n",
    "                        results[name] = evaluate_model(model, X_test, y_test)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error evaluating {name.upper()} model: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Save training history to CSV\n",
    "                    history_df = pd.DataFrame(history.history)\n",
    "                    history_df.to_csv(os.path.join(results_dir, f'{name}_history.csv'), index=False)\n",
    "                    \n",
    "                    # Plot training history\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.plot(history.history['loss'], label='Training Loss')\n",
    "                    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "                    plt.title(f'{name.upper()} Model Training History')\n",
    "                    plt.xlabel('Epoch')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.savefig(os.path.join(results_dir, f'{name}_training_history.png'))\n",
    "                    # plt.show()\n",
    "                    plt.close()  # Close the plot to free memory\n",
    "                    \n",
    "                    # Save evaluation results to a text file\n",
    "                    with open(os.path.join(results_dir, f'{name}_evaluation.txt'), 'w') as f:\n",
    "                        for metric_name, value in results[name].items():\n",
    "                            f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "                    \n",
    "                    # Plot predictions for Train, Val, and Test datasets and save the plots\n",
    "                    for dataset, data, true_values in zip(['Train', 'Val', 'Test'], \n",
    "                                                        [X_train, X_val, X_test], \n",
    "                                                        [y_train, y_val, y_test]):\n",
    "                        plot_predictions(\n",
    "                            model=model, \n",
    "                            X_data=data, \n",
    "                            y_data=true_values, \n",
    "                            label=name + ' ' + dataset, \n",
    "                            start=100, \n",
    "                            end=500\n",
    "                        )\n",
    "                        plt.savefig(os.path.join(results_dir, f'{name}_{dataset.lower()}_predictions.png'))\n",
    "                        # plt.show()\n",
    "                        plt.close()  # Close the plot to free memory\n",
    "                        \n",
    "                    print(\"********MODEL EVALUATION COMPLETE********\")\n",
    "            \n",
    "            print(f\"Results for loss function '{loss_function}' saved in '{results_dir}'.\")\n",
    "            \n",
    "        # Return results\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled error in execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best performance now\n",
    "1. GRU with *weighted loss* **(cycle_2/test_1)**\n",
    "   * mse: 0.8157\n",
    "   * mae: 0.6507\n",
    "   * false_negative_rate: 0.0000\n",
    "2. LSTM with *weighted loss* with *AdamW optimizer* **(cycle_2/test_3)**\n",
    "   * mse: 0.7435\n",
    "   * mae: 0.6114\n",
    "   * rmse: 0.8623\n",
    "   * false_negative_rate: 0.0000\n",
    "\n",
    "## overall best performance till now\n",
    "1. LSTM with *weighted loss* **(cycle_1/test_14)**\n",
    "   * mse: 0.7585\n",
    "   * mae: 0.6146\n",
    "   * false_negative_rate: 0.0000\n",
    "\n",
    "## All models have zero false negative rate: \n",
    "This means none of the models failed to predict rainfall events when they actually occurred, which is particularly important for weather forecasting.\n",
    "\n",
    "## Low values: \n",
    "All the error metrics are relatively low, suggesting that all three models performed reasonably well.\n",
    "\n",
    "## Training curves: \n",
    "All models show good convergence in their training histories, with both training and validation loss decreasing and stabilizing, indicating proper training without significant overfitting.\n",
    "\n",
    "## Visual patterns: \n",
    "From the graphs, all models seem to follow the general pattern of the actual rainfall values, but they struggle with extreme values (particularly low rainfall values where the actual data shows near-zero measurements)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
