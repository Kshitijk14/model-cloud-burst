{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model is Overfitting\n",
    "\n",
    "## Key Issues Observed:\n",
    "\n",
    "* Training loss is decreasing (from 138.5 to 2.1) but validation loss is increasing (from 4.6 to 14.7) - this is a clear sign of overfitting\n",
    "* Very low accuracy values (around 0.0002 for training and 0 for validation)\n",
    "* Increasing gap between training and validation RMSE\n",
    "\n",
    "## Suggestive Measures:\n",
    "\n",
    "1. **Regularization Techniques:**\n",
    "* Add dropout layers (start with 0.2-0.3)\n",
    "* Add L1/L2 regularization to your layers\n",
    "* Try batch normalization\n",
    "\n",
    "2. **Data Preprocessing:**\n",
    "* Normalize/standardize your input data\n",
    "* Check for and handle outliers\n",
    "* Ensure proper train/validation split methodology for time series (maintain temporal order)\n",
    "* Consider differencing or detrending if your data has strong trends\n",
    "\n",
    "3. **Model Architecture:**\n",
    "* Reduce model complexity if you have a deep network\n",
    "* If using LSTM/GRU, adjust the number of units\n",
    "* Try simpler architectures first (fewer layers)\n",
    "* Consider adding skip connections\n",
    "\n",
    "4. **Training Process:**\n",
    "* Implement early stopping (your model starts overfitting after epoch 4)\n",
    "* Reduce learning rate (try using learning rate scheduler)\n",
    "* Experiment with different batch sizes\n",
    "* Use K-fold cross-validation adapted for time series\n",
    "\n",
    "5. **Feature Engineering:**\n",
    "* Add relevant lag features\n",
    "* Include domain-specific features\n",
    "* Consider adding cyclical encodings for seasonal data\n",
    "* Create rolling statistics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>dew_point_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>cloud_cover_low</th>\n",
       "      <th>cloud_cover_mid</th>\n",
       "      <th>et0_fao_evapotranspiration</th>\n",
       "      <th>vapour_pressure_deficit</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_gusts_10m</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>96</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 01:00:00</th>\n",
       "      <td>2000-01-01 01:00:00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>95</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 02:00:00</th>\n",
       "      <td>2000-01-01 02:00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>95</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 03:00:00</th>\n",
       "      <td>2000-01-01 03:00:00</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 04:00:00</th>\n",
       "      <td>2000-01-01 04:00:00</td>\n",
       "      <td>12.9</td>\n",
       "      <td>75</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 19:00:00</th>\n",
       "      <td>2024-12-31 19:00:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>97</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 20:00:00</th>\n",
       "      <td>2024-12-31 20:00:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>96</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95</td>\n",
       "      <td>34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 21:00:00</th>\n",
       "      <td>2024-12-31 21:00:00</td>\n",
       "      <td>8.4</td>\n",
       "      <td>91</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "      <td>54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 22:00:00</th>\n",
       "      <td>2024-12-31 22:00:00</td>\n",
       "      <td>8.9</td>\n",
       "      <td>80</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>3.9</td>\n",
       "      <td>7.5</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 23:00:00</th>\n",
       "      <td>2024-12-31 23:00:00</td>\n",
       "      <td>8.1</td>\n",
       "      <td>79</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219168 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    time  temperature_2m  \\\n",
       "time                                                       \n",
       "2000-01-01 00:00:00  2000-01-01 00:00:00             4.0   \n",
       "2000-01-01 01:00:00  2000-01-01 01:00:00             4.1   \n",
       "2000-01-01 02:00:00  2000-01-01 02:00:00             4.0   \n",
       "2000-01-01 03:00:00  2000-01-01 03:00:00             6.5   \n",
       "2000-01-01 04:00:00  2000-01-01 04:00:00            12.9   \n",
       "...                                  ...             ...   \n",
       "2024-12-31 19:00:00  2024-12-31 19:00:00             8.4   \n",
       "2024-12-31 20:00:00  2024-12-31 20:00:00             8.4   \n",
       "2024-12-31 21:00:00  2024-12-31 21:00:00             8.4   \n",
       "2024-12-31 22:00:00  2024-12-31 22:00:00             8.9   \n",
       "2024-12-31 23:00:00  2024-12-31 23:00:00             8.1   \n",
       "\n",
       "                     relative_humidity_2m  dew_point_2m  rain  \\\n",
       "time                                                            \n",
       "2000-01-01 00:00:00                    96           3.3   0.0   \n",
       "2000-01-01 01:00:00                    95           3.3   0.0   \n",
       "2000-01-01 02:00:00                    95           3.2   0.0   \n",
       "2000-01-01 03:00:00                    92           5.3   0.0   \n",
       "2000-01-01 04:00:00                    75           8.6   0.0   \n",
       "...                                   ...           ...   ...   \n",
       "2024-12-31 19:00:00                    97           7.9   0.0   \n",
       "2024-12-31 20:00:00                    96           7.8   0.0   \n",
       "2024-12-31 21:00:00                    91           7.1   0.0   \n",
       "2024-12-31 22:00:00                    80           5.7   0.0   \n",
       "2024-12-31 23:00:00                    79           4.6   0.0   \n",
       "\n",
       "                     cloud_cover_low  cloud_cover_mid  \\\n",
       "time                                                    \n",
       "2000-01-01 00:00:00                0                0   \n",
       "2000-01-01 01:00:00                0                1   \n",
       "2000-01-01 02:00:00                0                1   \n",
       "2000-01-01 03:00:00                0                3   \n",
       "2000-01-01 04:00:00                0                0   \n",
       "...                              ...              ...   \n",
       "2024-12-31 19:00:00              100                5   \n",
       "2024-12-31 20:00:00               95               34   \n",
       "2024-12-31 21:00:00               57               54   \n",
       "2024-12-31 22:00:00               43                7   \n",
       "2024-12-31 23:00:00               37                7   \n",
       "\n",
       "                     et0_fao_evapotranspiration  vapour_pressure_deficit  \\\n",
       "time                                                                       \n",
       "2000-01-01 00:00:00                        0.00                     0.04   \n",
       "2000-01-01 01:00:00                        0.00                     0.04   \n",
       "2000-01-01 02:00:00                        0.01                     0.04   \n",
       "2000-01-01 03:00:00                        0.02                     0.07   \n",
       "2000-01-01 04:00:00                        0.13                     0.37   \n",
       "...                                         ...                      ...   \n",
       "2024-12-31 19:00:00                        0.00                     0.03   \n",
       "2024-12-31 20:00:00                        0.00                     0.04   \n",
       "2024-12-31 21:00:00                        0.00                     0.10   \n",
       "2024-12-31 22:00:00                        0.00                     0.23   \n",
       "2024-12-31 23:00:00                        0.00                     0.23   \n",
       "\n",
       "                     wind_speed_10m  wind_speed_100m  wind_gusts_10m  \n",
       "time                                                                  \n",
       "2000-01-01 00:00:00             3.8              3.6            11.5  \n",
       "2000-01-01 01:00:00             4.1              4.4            11.9  \n",
       "2000-01-01 02:00:00             3.8              4.5            11.9  \n",
       "2000-01-01 03:00:00             4.3              4.0            13.0  \n",
       "2000-01-01 04:00:00             2.0              2.5            13.0  \n",
       "...                             ...              ...             ...  \n",
       "2024-12-31 19:00:00             1.5              3.1             4.0  \n",
       "2024-12-31 20:00:00             2.7              5.0             6.5  \n",
       "2024-12-31 21:00:00             3.3              6.2             7.2  \n",
       "2024-12-31 22:00:00             3.9              7.5             9.7  \n",
       "2024-12-31 23:00:00             2.9              6.1             9.4  \n",
       "\n",
       "[219168 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../artifacts/dataset/01-hourly_historical_analyzed_data.csv')\n",
    "df = df.drop(columns=['hour', 'day', 'month', 'year'])\n",
    "\n",
    "df1 = df.copy()\n",
    "df1.index = pd.to_datetime(df1['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2000-01-09 04:00:00    0.1\n",
       "2000-01-11 02:00:00    0.2\n",
       "2000-01-11 03:00:00    0.3\n",
       "2000-01-12 04:00:00    0.2\n",
       "2000-01-12 05:00:00    0.6\n",
       "                      ... \n",
       "2024-12-28 04:00:00    1.1\n",
       "2024-12-28 05:00:00    1.3\n",
       "2024-12-28 06:00:00    0.3\n",
       "2024-12-28 10:00:00    0.1\n",
       "2024-12-28 11:00:00    0.1\n",
       "Name: Rain, Length: 35978, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain = df1['rain']\n",
    "rain_df = pd.DataFrame({'Rain': rain})\n",
    "\n",
    "rain_df[rain_df['Rain'] != 0]['Rain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data Pre-processing\n",
    "\n",
    "## 2.1 Convert: Time stamp -> Cyclic Signals\n",
    "\n",
    "### Summary:\n",
    "\n",
    "The cyclical component of a time series is a long-term variation in data that repeats in a systematic way over time. It's characterized by rises and falls that are not fixed in period, and are usually at least two years in duration. The cyclical component is often represented by a wave-shaped curve that shows alternating periods of expansion and contraction. \n",
    "\n",
    "Here are some examples of cyclical components in time series:\n",
    "* **Canadian lynx data:** This data shows population cycles of about 10 years. \n",
    "* **Stock market:** The stock market cycles between periods of high and low values, but there is no set amount of time between those fluctuations. \n",
    "* **Home prices:** There is a cyclical effect due to the market, but there is also a seasonal effect because most people would rather move in the summer.\n",
    "\n",
    "Cyclical components are different from seasonal components, which are variations that occur periodically in the data and are usually associated with changes in seasons, days of the week, or hours of the day.\n",
    "\n",
    "#### *Research/References:*\n",
    "\n",
    "1. [Components of Time Series Analysis](https://www.toppr.com/guides/business-mathematics-and-statistics/time-series-analysis/components-of-time-series/#:~:text=The%20variations%20in%20a%20time,called%20the%20'Business%20Cycle'.)\n",
    "2. [Cyclic and seasonal time series by Rob J Hyndman](https://robjhyndman.com/hyndsight/cyclicts/)\n",
    "3. [Time Series Analysis by Arief Wicaksono](https://medium.com/@ariefwcks303/time-series-analysis-bb61d1d1b3d5)\n",
    "\n",
    "### How to Convert:\n",
    "\n",
    "To convert a timestamp into a time series cyclic component, you need to extract cyclical features from the timestamp by calculating trigonometric functions like sine and cosine based on the relevant time unit (e.g., hour, day, month) within the cycle, effectively mapping the time point onto a circular representation where each cycle is represented by a full rotation on the unit circle; this is often done using the \"sinusoidal encoding\" method. \n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Convert timestamp to datetime object:** Use your programming language's datetime functions to convert the timestamp into a datetime object, allowing you to easily extract components like hour, day, month, etc.\n",
    "2. **Calculate cyclical features:**\n",
    "   * **Extract relevant time unit:** Depending on your analysis, extract the specific time unit that represents the cycle (e.g., hour for daily cycles, month for yearly cycles). \n",
    "   * **Normalize the unit:** Divide the extracted time unit by the maximum value within the cycle (e.g., divide hour by 23 to get a value between 0 and 1). \n",
    "   * **Apply sine and cosine functions:** Calculate the sine and cosine of the normalized time unit. These values will represent the cyclical component of your timestamp.\n",
    "\n",
    "#### *Research/References:*\n",
    "\n",
    "1. [Components of Time Series](https://ming-zhao.github.io/Business-Analytics/html/docs/time_series/components.html#:~:text=A%20seasonal%20behavior%20is%20very,seasonal%20(or%20cyclical)%20effects.)\n",
    "2. [Feature engineering of timestamp for time series analysis](https://datascience.stackexchange.com/questions/107215/feature-engineering-of-timestamp-for-time-series-analysis)\n",
    "3. [SQL: How can I generate a time series from timestamp data and calculate cumulative sums across different event types?](https://stackoverflow.com/questions/76295454/sql-how-can-i-generate-a-time-series-from-timestamp-data-and-calculate-cumulati)\n",
    "4. [Time Series Analysis Through Vectorization](https://www.pinecone.io/learn/time-series-vectors/)\n",
    "5. [Cyclical features in time series forecasting](https://skforecast.org/0.9.0/faq/cyclical-features-time-series#:~:text=Basis%20functions:%20Basis%20functions%20are,a%20piecewise%20combination%20of%20polynomials.)\n",
    "6. [Cyclical Encoding: An Alternative to One-Hot Encoding for Time Series Features](https://towardsdatascience.com/cyclical-encoding-an-alternative-to-one-hot-encoding-for-time-series-features-4db46248ebba#:~:text=It's%20fairly%20easy%20to%20transform,dt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rain</th>\n",
       "      <th>Seconds</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "      <th>rolling_mean_6h</th>\n",
       "      <th>rolling_std_6h</th>\n",
       "      <th>rolling_max_6h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.466848e+08</td>\n",
       "      <td>-5.461913e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004731</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.466884e+08</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.004014</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 02:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.466920e+08</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 03:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.466956e+08</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 04:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.466992e+08</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.735672e+09</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 20:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.735675e+09</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.735679e+09</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 22:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.735682e+09</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.009963</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-31 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.735686e+09</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219168 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Rain       Seconds       Day sin   Day cos  Year sin  \\\n",
       "time                                                                        \n",
       "2000-01-01 00:00:00   0.0  9.466848e+08 -5.461913e-12  1.000000 -0.004731   \n",
       "2000-01-01 01:00:00   0.0  9.466884e+08  2.588190e-01  0.965926 -0.004014   \n",
       "2000-01-01 02:00:00   0.0  9.466920e+08  5.000000e-01  0.866025 -0.003297   \n",
       "2000-01-01 03:00:00   0.0  9.466956e+08  7.071068e-01  0.707107 -0.002580   \n",
       "2000-01-01 04:00:00   0.0  9.466992e+08  8.660254e-01  0.500000 -0.001864   \n",
       "...                   ...           ...           ...       ...       ...   \n",
       "2024-12-31 19:00:00   0.0  1.735672e+09 -9.659258e-01  0.258819  0.007813   \n",
       "2024-12-31 20:00:00   0.0  1.735675e+09 -8.660254e-01  0.500000  0.008530   \n",
       "2024-12-31 21:00:00   0.0  1.735679e+09 -7.071068e-01  0.707107  0.009246   \n",
       "2024-12-31 22:00:00   0.0  1.735682e+09 -5.000000e-01  0.866025  0.009963   \n",
       "2024-12-31 23:00:00   0.0  1.735686e+09 -2.588190e-01  0.965926  0.010680   \n",
       "\n",
       "                     Year cos  rolling_mean_6h  rolling_std_6h  rolling_max_6h  \n",
       "time                                                                            \n",
       "2000-01-01 00:00:00  0.999989              NaN             NaN             NaN  \n",
       "2000-01-01 01:00:00  0.999992              NaN             NaN             NaN  \n",
       "2000-01-01 02:00:00  0.999995              NaN             NaN             NaN  \n",
       "2000-01-01 03:00:00  0.999997              NaN             NaN             NaN  \n",
       "2000-01-01 04:00:00  0.999998              NaN             NaN             NaN  \n",
       "...                       ...              ...             ...             ...  \n",
       "2024-12-31 19:00:00  0.999969              0.0             0.0             0.0  \n",
       "2024-12-31 20:00:00  0.999964              0.0             0.0             0.0  \n",
       "2024-12-31 21:00:00  0.999957              0.0             0.0             0.0  \n",
       "2024-12-31 22:00:00  0.999950              0.0             0.0             0.0  \n",
       "2024-12-31 23:00:00  0.999943              0.0             0.0             0.0  \n",
       "\n",
       "[219168 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process temporal features\n",
    "rain_df['Seconds'] = rain_df.index.map(pd.Timestamp.timestamp)\n",
    "\n",
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "\n",
    "# Add cyclical time features\n",
    "rain_df['Day sin'] = np.sin(rain_df['Seconds'] * (2 * np.pi / day))\n",
    "rain_df['Day cos'] = np.cos(rain_df['Seconds'] * (2 * np.pi / day))\n",
    "rain_df['Year sin'] = np.sin(rain_df['Seconds'] * (2 * np.pi / year))\n",
    "rain_df['Year cos'] = np.cos(rain_df['Seconds'] * (2 * np.pi / year))\n",
    "\n",
    "# Add rolling statistics\n",
    "rain_df['rolling_mean_6h'] = rain_df.iloc[:, 0].rolling(window=6).mean()\n",
    "rain_df['rolling_std_6h'] = rain_df.iloc[:, 0].rolling(window=6).std()\n",
    "rain_df['rolling_max_6h'] = rain_df.iloc[:, 0].rolling(window=6).max()\n",
    "\n",
    "rain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>dew_point_2m</th>\n",
       "      <th>cloud_cover_low</th>\n",
       "      <th>cloud_cover_mid</th>\n",
       "      <th>et0_fao_evapotranspiration</th>\n",
       "      <th>vapour_pressure_deficit</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_speed_100m</th>\n",
       "      <th>wind_gusts_10m</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "      <th>rolling_mean_6h</th>\n",
       "      <th>rolling_std_6h</th>\n",
       "      <th>rolling_max_6h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-01 00:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>96</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.461913e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004731</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 01:00:00</th>\n",
       "      <td>4.1</td>\n",
       "      <td>95</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.004014</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 02:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>95</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.003297</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 03:00:00</th>\n",
       "      <td>6.5</td>\n",
       "      <td>92</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.002580</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-01 04:00:00</th>\n",
       "      <td>12.9</td>\n",
       "      <td>75</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.001864</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     temperature_2m  relative_humidity_2m  dew_point_2m  \\\n",
       "time                                                                      \n",
       "2000-01-01 00:00:00             4.0                    96           3.3   \n",
       "2000-01-01 01:00:00             4.1                    95           3.3   \n",
       "2000-01-01 02:00:00             4.0                    95           3.2   \n",
       "2000-01-01 03:00:00             6.5                    92           5.3   \n",
       "2000-01-01 04:00:00            12.9                    75           8.6   \n",
       "\n",
       "                     cloud_cover_low  cloud_cover_mid  \\\n",
       "time                                                    \n",
       "2000-01-01 00:00:00                0                0   \n",
       "2000-01-01 01:00:00                0                1   \n",
       "2000-01-01 02:00:00                0                1   \n",
       "2000-01-01 03:00:00                0                3   \n",
       "2000-01-01 04:00:00                0                0   \n",
       "\n",
       "                     et0_fao_evapotranspiration  vapour_pressure_deficit  \\\n",
       "time                                                                       \n",
       "2000-01-01 00:00:00                        0.00                     0.04   \n",
       "2000-01-01 01:00:00                        0.00                     0.04   \n",
       "2000-01-01 02:00:00                        0.01                     0.04   \n",
       "2000-01-01 03:00:00                        0.02                     0.07   \n",
       "2000-01-01 04:00:00                        0.13                     0.37   \n",
       "\n",
       "                     wind_speed_10m  wind_speed_100m  wind_gusts_10m  Rain  \\\n",
       "time                                                                         \n",
       "2000-01-01 00:00:00             3.8              3.6            11.5   0.0   \n",
       "2000-01-01 01:00:00             4.1              4.4            11.9   0.0   \n",
       "2000-01-01 02:00:00             3.8              4.5            11.9   0.0   \n",
       "2000-01-01 03:00:00             4.3              4.0            13.0   0.0   \n",
       "2000-01-01 04:00:00             2.0              2.5            13.0   0.0   \n",
       "\n",
       "                          Day sin   Day cos  Year sin  Year cos  \\\n",
       "time                                                              \n",
       "2000-01-01 00:00:00 -5.461913e-12  1.000000 -0.004731  0.999989   \n",
       "2000-01-01 01:00:00  2.588190e-01  0.965926 -0.004014  0.999992   \n",
       "2000-01-01 02:00:00  5.000000e-01  0.866025 -0.003297  0.999995   \n",
       "2000-01-01 03:00:00  7.071068e-01  0.707107 -0.002580  0.999997   \n",
       "2000-01-01 04:00:00  8.660254e-01  0.500000 -0.001864  0.999998   \n",
       "\n",
       "                     rolling_mean_6h  rolling_std_6h  rolling_max_6h  \n",
       "time                                                                  \n",
       "2000-01-01 00:00:00              0.0             0.0             0.0  \n",
       "2000-01-01 01:00:00              0.0             0.0             0.0  \n",
       "2000-01-01 02:00:00              0.0             0.0             0.0  \n",
       "2000-01-01 03:00:00              0.0             0.0             0.0  \n",
       "2000-01-01 04:00:00              0.0             0.0             0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_df = rain_df.bfill()\n",
    "rain_df = rain_df.drop(['Seconds'], axis=1)\n",
    "\n",
    "df2 = pd.concat([df1, rain_df], axis=1)\n",
    "df2 = df2.drop(['time','rain'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create: Lagged Features\n",
    "\n",
    "### Summary:\n",
    "\n",
    "A lag feature in a time series is a feature that contains the value of a time series at a previous time point. The user sets the lag, or the number of periods in the past, for the feature. For example, a lag of 1 means the feature contains the previous time point value, while a lag of 3 means the feature contains the value three time points before.\n",
    "\n",
    "Lag features can be created by shifting the original data by one or more time steps. For example, if you have a daily time series of sales, you can create a lagged feature that shows the sales of the previous day, the same day last week, or the same day last year.\n",
    "\n",
    "In Python, you can create lag features using the pandas method shift. For example, X[my_variable].shift(freq=”1H”, axis=0) creates a new feature that contains lagged values of my_variable by one hour.\n",
    "\n",
    "The LagFeatures feature in Feature-engine has the same functionality as pandas shift(), but it can create multiple lags at the same time.\n",
    "\n",
    "#### *Research/References:*\n",
    "\n",
    "1. [What are lagged features?](https://www.hopsworks.ai/dictionary/lagged-features#:~:text=A%20lagged%20feature%20is%20created,at%20the%20current%20time%20point.)\n",
    "2. [How can you use lagged features to capture temporal dependencies in time series data?: LinkedIn](https://www.linkedin.com/advice/0/how-can-you-use-lagged-features-capture-temporal-ks4kc#:~:text=Lagged%20features%20are%20features%20that,the%20same%20day%20last%20year.)\n",
    "3. [LagFeatures: Automating lag feature creation](https://feature-engine.trainindata.com/en/1.8.x/user_guide/timeseries/forecasting/LagFeatures.html)\n",
    "4. [Time Series as Features by Ryan Halbrook and Alexis Cook](https://www.kaggle.com/code/ryanholbrook/time-series-as-features)\n",
    "5. [Introduction to feature engineering for time series forecasting by Francesca Lazzeri](https://medium.com/data-science-at-microsoft/introduction-to-feature-engineering-for-time-series-forecasting-620aa55fcab0)\n",
    "6. [Lagged features for time series forecasting: Scikit Learn](https://scikit-learn.org/1.5/auto_examples/applications/plot_time_series_lagged_features.html)\n",
    "7. [Lag features for time-series forecasting in AutoML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-automl-forecasting-lags?view=azureml-api-2)\n",
    "8. [Analyzing the Impact of Lagged Features in Time Series Forecasting: A Linear Regression Approach](https://cubed.run/blog/analyzing-the-impact-of-lagged-features-in-time-series-forecasting-a-linear-regression-approach-730aaa99dfd6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219162, 6, 18), (219162,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_X_y(df, window_size=6):\n",
    "    df_as_np = df.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(len(df_as_np) - window_size):\n",
    "        row = [r for r in df_as_np[i: i + window_size]]\n",
    "        X.append(row)\n",
    "        \n",
    "        label = df_as_np[i + window_size][0]\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 24 hour window\n",
    "X, y = df_to_X_y(df2, window_size=6)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Rolling Window Split\n",
    "\n",
    "### Summary:\n",
    "\n",
    "A \"rolling window split\" is a data splitting technique used primarily in time series analysis where a fixed-size \"window\" of data slides through the dataset, progressively moving forward in time, creating training sets that incorporate the temporal dependencies within the data, unlike a normal sequential split which simply divides the data into distinct, non-overlapping chunks in chronological order; essentially, a rolling window split allows the model to \"learn\" from the past data as it moves through the time series, while a sequential split treats each time period as completely independent.\n",
    "\n",
    "#### Key differences: \n",
    "\n",
    "1. **Window movement:** In a rolling window split, the training window \"rolls\" forward, meaning each new training set includes data from the previous window, whereas in a sequential split, the training set simply moves to the next time period without overlapping data from the previous set. \n",
    "2. **Temporal dependence:** A rolling window split is designed to capture the temporal dependencies within time series data, as the model is always learning on a recent \"window\" of data which is crucial for accurate predictions in time-sensitive scenarios. \n",
    "3. **Validation accuracy:** By considering the temporal relationships, rolling window splits often provide a more realistic evaluation of model performance on time series data compared to a simple sequential split. \n",
    "\n",
    "#### Example:\n",
    "\n",
    "* **Rolling Window Split:** Imagine predicting daily stock prices. With a rolling window of 30 days, the model would train on the first 30 days of data, then slide the window forward to include days 2-31, then 3-32, and so on, always considering recent historical data. \n",
    "* **Sequential Split:** In contrast, a sequential split might train on the first 30 days of data, then the next 30 days, completely ignoring the relationship between previous days and the current prediction.\n",
    "\n",
    "#### *Research/References:*\n",
    "\n",
    "1. [Time Series Splitting Techniques: Ensuring Accurate Model Validation by Mouad En-nasiry](https://medium.com/@mouadenna/time-series-splitting-techniques-ensuring-accurate-model-validation-5a3146db3088)\n",
    "2. [Splitting Your Data: Amazon Machine Learning AWS Docs.](https://docs.aws.amazon.com/machine-learning/latest/dg/splitting-types.html#:~:text=Sequentially%20Splitting%20Your%20Data%20A%20simple%20way,date%20or%20within%20a%20certain%20time%20range.)\n",
    "3. [Rolling-Window Analysis of Time-Series Models: Math Works](https://it.mathworks.com/help/econ/rolling-window-estimation-of-state-space-models.html)\n",
    "4. [[Q] What is the difference between sliding, rolling and expanding window in Time series forecasts?](https://www.reddit.com/r/statistics/comments/kxtzzx/q_what_is_the_difference_between_sliding_rolling/)\n",
    "5. [Difference between use cases of expanding and rolling window in backtesting](https://stats.stackexchange.com/questions/568814/difference-between-use-cases-of-expanding-and-rolling-window-in-backtesting)\n",
    "6. [Rolling or sliding window iterator?](https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator)\n",
    "7. [Windowing operations: Pandas](https://pandas.pydata.org/docs/user_guide/window.html)\n",
    "8. [Pandas Windowing Operations](https://pandas.pydata.org/pandas-docs/version/1.2/user_guide/window.html)\n",
    "9. [Time Based Cross Validation by Or Herman-Saffar](https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8)\n",
    "10. [Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis](https://arxiv.org/pdf/2307.14294)\n",
    "11. [TimeSeriesSplit: Scikit Learn](https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "12. [Rolling window time series prediction using MapReduce](https://www.researchgate.net/publication/282845273_Rolling_window_time_series_prediction_using_MapReduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((175329, 6, 18),\n",
       " (175329,),\n",
       " (21916, 6, 18),\n",
       " (21916,),\n",
       " (21917, 6, 18),\n",
       " (21917,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_time_series_data(X, y, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits time series data into training, validation, and testing sets sequentially.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Features (numpy array, DataFrame).\n",
    "    - y: Labels (numpy array, Series).\n",
    "    - train_ratio: Proportion of data for training (default 0.8).\n",
    "    - val_ratio: Proportion of data for validation (default 0.1).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_val, X_test, y_train, y_val, y_test: Sequentially split data.\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples for each split\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = train_end + int(n * val_ratio)\n",
    "\n",
    "    # Split data sequentially\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_time_series_data(X, y)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Standardization (Standard Normal Distribution) & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization completed!\n",
      "Scaling completed!\n"
     ]
    }
   ],
   "source": [
    "rain_training_mean = np.mean(X_train[:, :, 0])\n",
    "rain_training_std = np.std(X_train[:, :, 0])\n",
    "\n",
    "def preprocess_standardize(X):\n",
    "    X[:, :, 0] = (X[:, :, 0] - rain_training_mean) / rain_training_std\n",
    "    return X\n",
    "\n",
    "def preprocess__standardize_output(y):\n",
    "    # Check if y is 1D or 2D\n",
    "    if len(y.shape) == 2:  # If it's 2D (like a column vector), you can index it\n",
    "        y[:, 0] = (y[:, 0] - rain_training_mean) / rain_training_std\n",
    "    else:  # If it's 1D, you don't need the extra index\n",
    "        y = (y - rain_training_mean) / rain_training_std\n",
    "    return y\n",
    "\n",
    "preprocess_standardize(X_train)\n",
    "preprocess_standardize(X_val)\n",
    "preprocess_standardize(X_test)\n",
    "\n",
    "preprocess__standardize_output(y_train)\n",
    "preprocess__standardize_output(y_val)\n",
    "preprocess__standardize_output(y_test)\n",
    "\n",
    "print(\"Standardization completed!\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the 3D array to 2D (combine samples and timesteps)\n",
    "num_samples, num_timesteps, num_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "\n",
    "# Fit and transform the scaler on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train = X_train_scaled.reshape(num_samples, num_timesteps, num_features)\n",
    "\n",
    "# Repeat for validation and test datasets\n",
    "X_val_reshaped = X_val.reshape(-1, num_features)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)\n",
    "X_val = X_val_scaled.reshape(X_val.shape)\n",
    "\n",
    "X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "X_test = X_test_scaled.reshape(X_test.shape)\n",
    "\n",
    "print(\"Scaling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architectures\n",
    "def create_cnn_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(dropout_rate),\n",
    "            Flatten(),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_cnn_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_lstm_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4), recurrent_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            LSTM(64, return_sequences=False, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_lstm_model: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_gru_model(input_shape, dropout_rate=0.3):\n",
    "    try:\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=input_shape),\n",
    "            \n",
    "            GRU(128, return_sequences=True, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            GRU(64, return_sequences=False, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "            Dropout(dropout_rate),\n",
    "            \n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_gru_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Model Training\n",
    "\n",
    "## Loss Functions for Time Series Forecasting\n",
    "\n",
    "1. **Mean Squared Error (MSE):**\n",
    "   * **Use Case:** MSE is widely used for regression tasks, including time series forecasting. It penalizes larger errors more heavily, which can be beneficial if you want to minimize large deviations from the true values.\n",
    "   * **Pros:** Simple to implement and interpret; works well when errors are normally distributed.\n",
    "   * **Cons:** Sensitive to outliers, which can skew the results.\n",
    "\n",
    "2. **Mean Absolute Error (MAE):**\n",
    "   * **Use Case:** MAE is another common choice that measures the average magnitude of errors without considering their direction. It is less sensitive to outliers compared to MSE.\n",
    "   * **Pros:** More robust to outliers; provides a linear score that is easier to interpret.\n",
    "   * **Cons:** Does not penalize larger errors as heavily as MSE.\n",
    "\n",
    "3. **Huber Loss:**\n",
    "   * **Use Case:** Huber loss combines the properties of MSE and MAE. It behaves like MSE for small errors and like MAE for large errors, making it robust to outliers.\n",
    "   * **Pros:** Balances sensitivity to outliers and provides a smooth gradient.\n",
    "   * **Cons:** Requires tuning of a threshold parameter.\n",
    "\n",
    "4. **Custom Loss Functions:**\n",
    "   * **Use Case:** If your forecasting task has specific requirements (e.g., you want to penalize false negatives more heavily), a custom loss function like the one you provided can be beneficial.\n",
    "   * **Pros:** Tailored to your specific needs; can emphasize certain types of errors.\n",
    "   * **Cons:** More complex to implement and may require more experimentation to tune effectively.\n",
    "\n",
    "5. **Weighted Loss Functions:**\n",
    "   * **Use Case:** If your time series data has imbalances (e.g., certain periods are more critical), using a weighted loss function can help emphasize those periods.\n",
    "   * **Pros:** Allows for flexibility in penalizing different types of errors based on their importance.\n",
    "   * **Cons:** Requires careful selection of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    try:\n",
    "        return tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in huber_loss: {e}\")\n",
    "        raise\n",
    "\n",
    "# Weighted mean squared error\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    try:\n",
    "        weights = tf.where(y_pred < y_true, 2.0, 1.0) # Penalize underestimation more heavily (false negatives)\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        return tf.reduce_mean(weights * squared_difference)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in weighted_mse: {e}\")\n",
    "        raise\n",
    "\n",
    "# Custom loss function to penalize false negatives more heavily\n",
    "def custom_loss(y_true, y_pred):\n",
    "    try:\n",
    "        squared_difference = tf.square(y_true - y_pred)\n",
    "        mse = tf.reduce_mean(squared_difference, axis=-1)\n",
    "        penalty = tf.where(y_pred < y_true, 2.0, 1.0) # Similar Penalty Logic\n",
    "        return mse * penalty\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_loss: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with callbacks\n",
    "def train_model(model, X_train, y_train, X_val, y_val, model_path, \n",
    "                batch_size=32, epochs=5, patience=15, loss_function='mse', learning_rate=1e-3):\n",
    "    try:\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),\n",
    "            ModelCheckpoint(model_path, save_best_only=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Select the loss function based on the input parameter\n",
    "        if loss_function == 'mse':\n",
    "            loss = 'mean_squared_error'\n",
    "        elif loss_function == 'mae':\n",
    "            loss = 'mean_absolute_error'\n",
    "        elif loss_function == 'huber':\n",
    "            loss = huber_loss\n",
    "        elif loss_function == 'weighted_mse':\n",
    "            loss = weighted_mse\n",
    "        elif loss_function == 'custom':\n",
    "            loss = custom_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function specified.\")\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            loss=loss,\n",
    "            metrics=[\n",
    "                'mse', \n",
    "                'mae',\n",
    "                'root_mean_squared_error'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with focus on false negatives\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    try:\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        mse = np.mean((y_test - predictions.flatten())**2)\n",
    "        mae = np.mean(np.abs(y_test - predictions.flatten()))\n",
    "        \n",
    "        # Calculate false negative rate\n",
    "        binary_actual = y_test > threshold\n",
    "        binary_pred = predictions.flatten() > threshold\n",
    "        false_negatives = np.sum((binary_actual == True) & (binary_pred == False))\n",
    "        false_negative_rate = false_negatives / np.sum(binary_actual)\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'false_negative_rate': false_negative_rate\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, X_data, y_data, label, start=50, end=500, ylabel='Rainfall (mm)', title_suffix=''):\n",
    "    \"\"\"\n",
    "    Plots predictions vs actual values for a given model and dataset.\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained model to use for predictions.\n",
    "    - X_data: Input data for predictions.\n",
    "    - y_data: Actual target values.\n",
    "    - label: A string indicating the dataset (e.g., 'Train', 'Validation', 'Test').\n",
    "    - start, end: Range of data points to visualize (default: 50 to 500).\n",
    "    - ylabel: Label for the y-axis (default: 'Rainfall (mm)').\n",
    "    - title_suffix: Additional suffix for the title (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing the predictions and actual values.\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_data).flatten()\n",
    "\n",
    "    # Create a DataFrame to store results\n",
    "    results_df = pd.DataFrame(data={f'{label} Predictions': predictions, 'Actual Values': y_data})\n",
    "    print(results_df)\n",
    "\n",
    "    # Plot the predictions and actual values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df[f'{label} Predictions'][start:end], label=f'{label} Predictions', color='blue', linestyle='-')\n",
    "    plt.plot(results_df['Actual Values'][start:end], label='Actual Values', color='orange', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time Stamps', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.title(f'{label} Predictions vs Actual Values {title_suffix}', fontsize=14)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    try:\n",
    "        # Define loss functions to iterate over\n",
    "        loss_functions = ['mse', 'mae', 'huber', 'weighted_mse', 'custom']\n",
    "        \n",
    "        for loss_function in loss_functions:\n",
    "            # Create a directory for the current loss function\n",
    "            results_dir = f'../artifacts/results/{loss_function}'\n",
    "            os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Train models\n",
    "        models = {\n",
    "            'cnn': create_cnn_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            'lstm': create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            'gru': create_gru_model(input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name.upper()} model with {loss_function.upper()} loss...\")\n",
    "            \n",
    "            try:\n",
    "                history = train_model(\n",
    "                    model, X_train, y_train, X_val, y_val,\n",
    "                    f'../artifacts/models/multivariate/model_{name}.keras', \n",
    "                    epochs=5, loss_function=loss_function, learning_rate=1e-3\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name.upper()} model: {e}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                results[name] = evaluate_model(model, X_test, y_test)\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {name.upper()} model: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Save training history to CSV\n",
    "            history_df = pd.DataFrame(history.history)\n",
    "            history_df.to_csv(os.path.join(results_dir, f'{name}_history.csv'), index=False)\n",
    "            \n",
    "            # Plot training history\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title(f'{name.upper()} Model Training History')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(results_dir, f'{name}_training_history.png'))\n",
    "            plt.close()  # Close the plot to free memory\n",
    "            \n",
    "            # Save evaluation results to a text file\n",
    "            with open(os.path.join(results_dir, f'{name}_evaluation.txt'), 'w') as f:\n",
    "                for metric_name, value in results[name].items():\n",
    "                    f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "            \n",
    "            # Plot predictions for Train, Val, and Test datasets and save the plots\n",
    "                for dataset, data, true_values in zip(['Train', 'Val', 'Test'], \n",
    "                                                       [X_train, X_val, X_test], \n",
    "                                                       [y_train, y_val, y_test]):\n",
    "                    plot_predictions(\n",
    "                        model=model, \n",
    "                        X_data=data, \n",
    "                        y_data=true_values, \n",
    "                        label=dataset, \n",
    "                        start=100, \n",
    "                        end=500\n",
    "                    )\n",
    "                    plt.savefig(os.path.join(results_dir, f'{name}_{dataset.lower()}_predictions.png'))\n",
    "                    plt.close()  # Close the plot to free memory\n",
    "        \n",
    "        print(f\"Results for loss function '{loss_function}' saved in '{results_dir}'.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results = main()\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name.upper()}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unhandled error in execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Interpretation\n",
    "\n",
    "1. **Mean Squared Error (MSE):** Lower values indicate better performance, as MSE measures the average squared difference between predicted and actual values.\n",
    "2. **Mean Absolute Error (MAE):** Similar to MSE, lower values are better. MAE measures the average absolute difference between predicted and actual values.\n",
    "3. **False Negative Rate (FNR):** A lower false negative rate is better, as it indicates that the model is correctly identifying positive cases. A high FNR means the model is missing many positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Results:\n",
    "\n",
    "1. **LSTM:** \n",
    "* *mse:* 0.5540 \n",
    "* *mae:* 0.1721 \n",
    "* *false_negative_rate:* 0.5877\n",
    "\n",
    "2. **GRU:** \n",
    "* *mse:* 0.4852 \n",
    "* *mae:* 0.1671 \n",
    "* *false_negative_rate:* 0.4767\n",
    "\n",
    "3. **CNN:** \n",
    "* *mse:* 0.4841 \n",
    "* *mae:* 0.1733 \n",
    "* *false_negative_rate:* 0.5009\n",
    "\n",
    "4. **CNN_2:** \n",
    "* *mse:* 1.1112 \n",
    "* *mae:* 0.8054 \n",
    "* *false_negative_rate:* 0.0000\n",
    "\n",
    "## Summary of Model Performance\n",
    "\n",
    "1. **CNN:** Best *MSE (0.4841)* but relatively high *FNR (0.5009)*.\n",
    "2. **GRU:** Best *MAE (0.1671)* and a lower* FNR (0.4767)* compared to LSTM and CNN.\n",
    "3. **LSTM:** Higher MSE and MAE than GRU and CNN, with the highest* FNR (0.5877)*.\n",
    "4. **CNN_2:** Poor overall performance with the highest *MSE (1.1112)* and *MAE (0.8054)*, despite a perfect *FNR (0.0000)*.\n",
    "\n",
    "### Conclusion *(Based on the evaluation metrics)*:\n",
    "\n",
    "* **Best Overall Model:** GRU seems to be the best choice overall, as it has the lowest MAE and a competitive FNR.\n",
    "* **Best for MSE:** CNN has the lowest MSE, but its FNR is higher than that of the GRU.\n",
    "* **Best for False Negatives:** CNN_2 has a perfect FNR, but its overall prediction quality is poor.\n",
    "\n",
    "### Recommendation\n",
    "If your primary concern is minimizing average prediction error, the GRU is the best choice. If you are particularly concerned about false negatives (e.g., in a medical diagnosis context), you might want to investigate the CNN_2 further, despite its poor overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
